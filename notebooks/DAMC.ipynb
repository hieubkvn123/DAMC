{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munchong/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class UserAutoEncoder(tf.keras.Model):\n",
    "    def __init__(self, input_feature_len, latent_dim):\n",
    "        super(UserAutoEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_feature_len = input_feature_len\n",
    "        \n",
    "        # Define encoder layers\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(100 * self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10 * self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "        ])\n",
    "\n",
    "        # Define decoder layers\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10 * self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(100 * self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(self.input_feature_len, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, inputs, training=False):\n",
    "        return self.encoder(inputs, training=training)\n",
    "\n",
    "class ItemAutoEncoder(tf.keras.Model):\n",
    "    def __init__(self, input_feature_len, latent_dim):\n",
    "        super(ItemAutoEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_feature_len = input_feature_len\n",
    "        \n",
    "        # Define encoder layers\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(100 * self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10 * self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "        ])\n",
    "\n",
    "        # Define decoder layers\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10 * self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(100 * self.latent_dim, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(self.input_feature_len, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, inputs, training=False):\n",
    "        return self.encoder(inputs, training=training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "# from sklearn.decomposition import PCA\n",
    "from scipy.linalg import svd\n",
    "from tensorflow.keras import Model\n",
    "# from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "class XMY:\n",
    "    def __init__(self, latent_side_info, M_max_rank, lambda_M, step, dataset_name, p_value, dtype=tf.float32):\n",
    "        self.latent_side_info = latent_side_info\n",
    "        self.M_max_rank = M_max_rank\n",
    "        self.lambda_M = lambda_M\n",
    "        self.step = step\n",
    "        self.dataset_name = dataset_name\n",
    "        self.p_value = p_value\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Path to save/load weights\n",
    "        self.autoencoder_X_weights_path = f'real_datasets/{dataset_name}/Weighted_XMY/p={p_value}/latent_dim={latent_side_info}/autoencoder_X_weights.h5'\n",
    "        self.autoencoder_Y_weights_path = f'real_datasets/{dataset_name}/Weighted_XMY/p={p_value}/latent_dim={latent_side_info}/autoencoder_Y_weights.h5'\n",
    "\n",
    "    def _check_weights(self, path):\n",
    "        \"\"\"Check if weights file exists at the given path.\"\"\"\n",
    "        return os.path.exists(path)\n",
    "\n",
    "    def fit(self, R_bar_train, R_bar_val, R_bar_test, R_train, R_val, R_test, tensorboard_dir=None):\n",
    "        tf.random.set_seed(42)\n",
    "\n",
    "        #matrix_processor = MatrixProcessor()\n",
    "        #X, Y = matrix_processor.compute_svd(R_bar_train, self.latent_side_info)\n",
    "        #R_bar_train = matrix_processor.compute_terms(R_bar_train)\n",
    "\n",
    "        # Initialize M\n",
    "        self.U, self.V = self._init_low_rank_matrices()\n",
    "\n",
    "        #matrix_processor = MatrixProcessor()\n",
    "        #user_matrix_multiplier = matrix_processor.compute_terms(R_bar_train)\n",
    "\n",
    "        batch_size = 32\n",
    "        #user_dataset = tf.data.Dataset.from_tensor_slices((R_bar_train, R_bar_train, user_matrix_multiplier))\n",
    "        user_dataset = tf.data.Dataset.from_tensor_slices((R_bar_train, R_bar_train))\n",
    "        user_dataset = user_dataset.shuffle(buffer_size=100).batch(batch_size)\n",
    "\n",
    "        ae_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "        M_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "        #print('Latent side info :', self.latent_side_info)\n",
    "        self.autoencoder_X = UserAutoEncoder(R_bar_train.shape[1], self.latent_side_info)\n",
    "        self.autoencoder_X.compile(optimizer=ae_optimizer, loss='binary_crossentropy')\n",
    "\n",
    "        #item_matrix_multiplier = matrix_processor.compute_terms(tf.transpose(R_bar_train))\n",
    "        item_dataset = tf.data.Dataset.from_tensor_slices((tf.transpose(R_bar_train), tf.transpose(R_bar_train)))\n",
    "        item_dataset = item_dataset.shuffle(buffer_size=100).batch(batch_size)\n",
    "\n",
    "        self.autoencoder_Y = ItemAutoEncoder(R_bar_train.shape[0], self.latent_side_info)\n",
    "        self.autoencoder_Y.compile(optimizer=ae_optimizer, loss='binary_crossentropy')\n",
    "\n",
    "        # # # Initialize autoencoder\n",
    "        # self.autoencoder_X = UserAutoEncoder(R_bar_train.shape[1], self.latent_side_info)\n",
    "        # self.autoencoder_Y = ItemAutoEncoder(R_bar_train.shape[0], self.latent_side_info)\n",
    "\n",
    "        # ae_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        # M_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "        # self.autoencoder_X.compile(optimizer=ae_optimizer, loss='binary_crossentropy')\n",
    "        # self.autoencoder_Y.compile(optimizer=ae_optimizer, loss='binary_crossentropy')\n",
    "\n",
    "        # Dummy forward pass to build the model (ensure weights are created)\n",
    "        _ = self.autoencoder_X(R_bar_train)\n",
    "        _ = self.autoencoder_Y(tf.transpose(R_bar_train))\n",
    "\n",
    "        # Check if weights exist for autoencoder_X\n",
    "        if self._check_weights(self.autoencoder_X_weights_path):\n",
    "            self.autoencoder_X.load_weights(self.autoencoder_X_weights_path)\n",
    "        else:\n",
    "            self.autoencoder_X.fit(user_dataset, epochs=300, verbose=0, batch_size=batch_size)\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(os.path.dirname(self.autoencoder_X_weights_path), exist_ok=True)\n",
    "\n",
    "            self.autoencoder_X.save_weights(self.autoencoder_X_weights_path)\n",
    "\n",
    "        # Check if weights exist for autoencoder_Y\n",
    "        if self._check_weights(self.autoencoder_Y_weights_path):\n",
    "            self.autoencoder_Y.load_weights(self.autoencoder_Y_weights_path)\n",
    "        else:\n",
    "            self.autoencoder_Y.fit(item_dataset, epochs=300, verbose=0, batch_size=batch_size)\n",
    "\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(os.path.dirname(self.autoencoder_Y_weights_path), exist_ok=True)\n",
    "\n",
    "            self.autoencoder_Y.save_weights(self.autoencoder_Y_weights_path)\n",
    "\n",
    "\n",
    "        # # Check if weights exist for autoencoder_X\n",
    "        # if self._check_weights(self.autoencoder_X_weights_path):\n",
    "        #     self.autoencoder_X.load_weights(self.autoencoder_X_weights_path)\n",
    "        # else:\n",
    "        #     self.autoencoder_X.fit(R_bar_train, R_bar_train, epochs=100, verbose=0)\n",
    "\n",
    "        #     # Ensure the directory exists\n",
    "        #     os.makedirs(os.path.dirname(self.autoencoder_X_weights_path), exist_ok=True)\n",
    "\n",
    "        #     self.autoencoder_X.save_weights(self.autoencoder_X_weights_path)\n",
    "\n",
    "        # # Check if weights exist for autoencoder_Y\n",
    "        # if self._check_weights(self.autoencoder_Y_weights_path):\n",
    "        #     self.autoencoder_Y.load_weights(self.autoencoder_Y_weights_path)\n",
    "        # else:\n",
    "        #     self.autoencoder_Y.fit(tf.transpose(R_bar_train), tf.transpose(R_bar_train), epochs=100, verbose=0)\n",
    "\n",
    "        #     # Ensure the directory exists\n",
    "        #     os.makedirs(os.path.dirname(self.autoencoder_Y_weights_path), exist_ok=True)\n",
    "\n",
    "        #     self.autoencoder_Y.save_weights(self.autoencoder_Y_weights_path)\n",
    "\n",
    "        # Define terms\n",
    "        reg_normalizer = tf.math.sqrt(tf.cast(R_bar_train.shape[0] * R_bar_train.shape[1], tf.float32))\n",
    "\n",
    "        train_mask = self._create_mask(R_train)\n",
    "        val_mask = self._create_mask(R_val)\n",
    "        test_mask = self._create_mask(R_test)\n",
    "\n",
    "        #Create tensorboard dir\n",
    "        #os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "        #writer = tf.summary.create_file_writer(tensorboard_dir)\n",
    "\n",
    "        # Initialize high minimums for the best RMSEs\n",
    "        best_train_rmse = float('inf')\n",
    "        best_val_rmse = float('inf')\n",
    "        best_test_rmse = float('inf')\n",
    "\n",
    "        # Early stopping parameters\n",
    "        patience = 200 # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "        # Initialize variables\n",
    "        patience_counter = 0\n",
    "        train_rmse_window = []\n",
    "        val_rmse_window = []\n",
    "        test_rmse_window = []\n",
    "\n",
    "        for i in range(self.step):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Extract latent vectors X and Y\n",
    "                X = self.autoencoder_X.encode(R_bar_train)\n",
    "                #print('X Shape :', X.shape)\n",
    "                Y = self.autoencoder_Y.encode(tf.transpose(R_bar_train))\n",
    "                #print('Y Shape :', Y.shape)\n",
    "\n",
    "                # Compute prediction term - XMY\n",
    "                XMY = tf.matmul(tf.matmul(X, tf.matmul(self.U, tf.transpose(self.V))), tf.transpose(Y))\n",
    "\n",
    "                # Compute squared loss\n",
    "                train_loss = tf.reduce_sum(train_mask * tf.math.squared_difference(R_train, XMY)) / tf.reduce_sum(train_mask)\n",
    "                val_loss = tf.reduce_sum(val_mask * tf.math.squared_difference(R_val, XMY)) / tf.reduce_sum(val_mask)\n",
    "                test_loss = tf.reduce_sum(test_mask * tf.math.squared_difference(R_test, XMY)) / tf.reduce_sum(test_mask)\n",
    "\n",
    "                # Compute regularization loss\n",
    "                loss_U = self.lambda_M * tf.reduce_sum(tf.math.square(self.U)) / reg_normalizer\n",
    "                loss_V = self.lambda_M * tf.reduce_sum(tf.math.square(self.V)) / reg_normalizer\n",
    "\n",
    "                # # Compute autoencoder loss\n",
    "                # loss_autoencoder_X = tf.reduce_mean(tf.keras.losses.mean_squared_error(R_bar_train, self.autoencoder_X(R_bar_train)))\n",
    "                # loss_autoencoder_Y = tf.reduce_mean(tf.keras.losses.mean_squared_error(tf.transpose(R_bar_train), self.autoencoder_Y(tf.transpose(R_bar_train))))\n",
    "\n",
    "                loss_autoencoder_X = tf.reduce_mean(tf.keras.losses.binary_crossentropy(R_bar_train, self.autoencoder_X(R_bar_train)))\n",
    "                loss_autoencoder_Y = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.transpose(R_bar_train), self.autoencoder_Y(tf.transpose(R_bar_train))))\n",
    "\n",
    "                # # For autoencoder_X\n",
    "                # x_pred = self.autoencoder_X(R_bar_train)\n",
    "                # loss_autoencoder_X = (self.autoencoder_X.compute_custom_loss(R_bar_train, x_pred, user_matrix_multiplier) / 1500)\n",
    "\n",
    "                # # For autoencoder_Y\n",
    "                # y_true_transposed = tf.transpose(R_bar_train)\n",
    "                # y_pred_transposed = self.autoencoder_Y(y_true_transposed)\n",
    "                # loss_autoencoder_Y = (self.autoencoder_Y.compute_custom_loss(y_true_transposed, y_pred_transposed, item_matrix_multiplier) / 1500)\n",
    "\n",
    "                # Total train loss\n",
    "                total_loss = train_loss + loss_U + loss_V + loss_autoencoder_X + loss_autoencoder_Y\n",
    "\n",
    "            # Do backpropagation\n",
    "            M_grads = tape.gradient(total_loss, [self.U, self.V])\n",
    "            M_optimizer.apply_gradients(zip(M_grads, [self.U, self.V]))\n",
    "\n",
    "            autoencoder_variables = self.autoencoder_X.trainable_variables + self.autoencoder_Y.trainable_variables\n",
    "            ae_grads = tape.gradient(total_loss, autoencoder_variables)\n",
    "            ae_optimizer.apply_gradients(zip(ae_grads, autoencoder_variables))\n",
    "\n",
    "            del tape\n",
    "\n",
    "            # Compute rmse\n",
    "            train_rmse = tf.math.sqrt(train_loss)\n",
    "            val_rmse = tf.math.sqrt(val_loss)\n",
    "            test_rmse = tf.math.sqrt(test_loss)\n",
    "\n",
    "            print('train_rmse :', train_rmse)\n",
    "\n",
    "            # If the current val rmse is less than the minimum in val rmse window, reset the patience counter\n",
    "            if i != 0 and val_rmse < min(val_rmse_window):\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                #If the val rmse didn't improve, increment patience counter\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Stop training if patience exceeded\n",
    "            if patience_counter > patience:\n",
    "                break\n",
    "\n",
    "            # Add current RMSEs to the windows\n",
    "            train_rmse_window.append(train_rmse)\n",
    "            val_rmse_window.append(val_rmse)\n",
    "            test_rmse_window.append(test_rmse)\n",
    "\n",
    "            # # Tensorboard logging\n",
    "            # with writer.as_default():\n",
    "            #     # Log rmse\n",
    "            #     tf.summary.scalar('train_rmse', train_rmse, i)\n",
    "            #     tf.summary.scalar('val_rmse', val_rmse, i)\n",
    "            #     tf.summary.scalar('test_rmse', test_rmse, i)\n",
    "\n",
    "            #     # Log autoencoder losses\n",
    "            #     tf.summary.scalar('User Autoencoder Loss', loss_autoencoder_X, i)\n",
    "            #     tf.summary.scalar('Item Autoencoder Loss', loss_autoencoder_Y, i)\n",
    "\n",
    "        # After training is done or patience is exceeded, get the best RMSE\n",
    "        min_val_index = val_rmse_window.index(min(val_rmse_window))\n",
    "\n",
    "        best_train_rmse = train_rmse_window[min_val_index]\n",
    "        best_val_rmse = val_rmse_window[min_val_index]\n",
    "        best_test_rmse = test_rmse_window[min_val_index]\n",
    "\n",
    "        return best_train_rmse, best_val_rmse, best_test_rmse\n",
    "\n",
    "\n",
    "    def _create_mask(self, X):\n",
    "        mask_tf = tf.where(X == 0, tf.zeros_like(X), 1)\n",
    "\n",
    "        return mask_tf\n",
    "\n",
    "    def _init_low_rank_matrices(self):\n",
    "        # Initialize low-rank matrices randomly\n",
    "        U = tf.Variable(tf.random.normal([self.latent_side_info, self.M_max_rank]), dtype=tf.float32)\n",
    "        V = tf.Variable(tf.random.normal([self.latent_side_info, self.M_max_rank]), dtype=tf.float32)\n",
    "\n",
    "        #Z_1 = tf.Variable(tf.linalg.qr(tf.random.normal([R.shape[0], self.Z_max_rank]))[0], dtype=tf.float32)\n",
    "        #Z_2 = tf.Variable(tf.linalg.qr(tf.random.normal([R.shape[1], self.Z_max_rank]))[0], dtype=tf.float32)\n",
    "\n",
    "        return U, V#, Z_1, Z_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 11:52:46.550354: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from utils.data_utils import prepare_data  # Assuming this function exists in your utils module\n",
    "\n",
    "dataset_name = 'ml_100k'\n",
    "p_value = 0.0\n",
    "\n",
    "R_bar_train, R_bar_val, R_bar_test, R_train, R_val, R_test = prepare_data(dataset_name, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(943, 1682), dtype=float32, numpy=\n",
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_bar_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(943, 1682), dtype=float32, numpy=\n",
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'JointModel'\n",
    "latent_side_info = 8\n",
    "lambda_ = 10\n",
    "is_weighted_implicit =False\n",
    "latent_M = 4\n",
    "step = 10000\n",
    "lr_M = 0.01\n",
    "lr_ae = 0.01\n",
    "\n",
    "config={\n",
    "    'dataset': dataset_name,\n",
    "    'p_value': p_value,\n",
    "    'latent_dim': latent_side_info,\n",
    "    'M_max_rank': latent_M,\n",
    "    'lambda_M': lambda_,\n",
    "    'step': step,\n",
    "    #'num_layer': num_layer,\n",
    "    'lr_M': lr_M,\n",
    "    'lr_ae': lr_ae,\n",
    "    'embeddings_epochs': 100,\n",
    "    'is_weighted_implicit': is_weighted_implicit,\n",
    "    'droputout': 0.2,\n",
    "    'batch_norm': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'dataset': 'ml_100k', 'p_value': 0.0, 'latent_dim': 8, 'M_max_rank': 4, 'lambda_M': 10, 'step': 10000, 'lr_M': 0.01, 'lr_ae': 0.01, 'embeddings_epochs': 100, 'is_weighted_implicit': False, 'droputout': 0.2, 'batch_norm': True}\n"
     ]
    }
   ],
   "source": [
    "from models.joint_model import JointModel\n",
    "\n",
    "model = JointModel(config, dataset_name, 'Weighted_XMY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 8)\n",
      "(1682, 8)\n",
      "Epoch 1 train_loss : 38.77296447753906\n",
      "Epoch 2 train_loss : 25.995100021362305\n",
      "Epoch 3 train_loss : 17.991588592529297\n",
      "Epoch 4 train_loss : 15.652216911315918\n",
      "Epoch 5 train_loss : 12.384057998657227\n",
      "Epoch 6 train_loss : 11.95546817779541\n",
      "Epoch 7 train_loss : 10.168505668640137\n",
      "Epoch 8 train_loss : 8.919242858886719\n",
      "Epoch 9 train_loss : 9.494385719299316\n",
      "Epoch 10 train_loss : 8.015703201293945\n",
      "Epoch 11 train_loss : 7.2010416984558105\n",
      "Epoch 12 train_loss : 6.681835174560547\n",
      "Epoch 13 train_loss : 6.403016090393066\n",
      "Epoch 14 train_loss : 6.087214469909668\n",
      "Epoch 15 train_loss : 5.964330196380615\n",
      "Epoch 16 train_loss : 5.9408721923828125\n",
      "Epoch 17 train_loss : 5.4376749992370605\n",
      "Epoch 18 train_loss : 5.495115280151367\n",
      "Epoch 19 train_loss : 5.595522403717041\n",
      "Epoch 20 train_loss : 5.484116554260254\n",
      "Epoch 21 train_loss : 5.092400074005127\n",
      "Epoch 22 train_loss : 4.721324443817139\n",
      "Epoch 23 train_loss : 4.60207462310791\n",
      "Epoch 24 train_loss : 4.589034557342529\n",
      "Epoch 25 train_loss : 4.6897101402282715\n",
      "Epoch 26 train_loss : 4.543811798095703\n",
      "Epoch 27 train_loss : 4.343938827514648\n",
      "Epoch 28 train_loss : 4.204866886138916\n",
      "Epoch 29 train_loss : 4.1229777336120605\n",
      "Epoch 30 train_loss : 4.083824634552002\n",
      "Epoch 31 train_loss : 3.9473843574523926\n",
      "Epoch 32 train_loss : 3.9503300189971924\n",
      "Epoch 33 train_loss : 4.052694320678711\n",
      "Epoch 34 train_loss : 4.017477989196777\n",
      "Epoch 35 train_loss : 3.924024820327759\n",
      "Epoch 36 train_loss : 3.943113327026367\n",
      "Epoch 37 train_loss : 3.8034427165985107\n",
      "Epoch 38 train_loss : 3.7631964683532715\n",
      "Epoch 39 train_loss : 3.691518783569336\n",
      "Epoch 40 train_loss : 3.7410285472869873\n",
      "Epoch 41 train_loss : 3.7273037433624268\n",
      "Epoch 42 train_loss : 3.589916229248047\n",
      "Epoch 43 train_loss : 3.620551109313965\n",
      "Epoch 44 train_loss : 3.600693464279175\n",
      "Epoch 45 train_loss : 3.477799415588379\n",
      "Epoch 46 train_loss : 3.460517168045044\n",
      "Epoch 47 train_loss : 3.4291367530822754\n",
      "Epoch 48 train_loss : 3.461738348007202\n",
      "Epoch 49 train_loss : 3.4066922664642334\n",
      "Epoch 50 train_loss : 3.372530221939087\n",
      "Epoch 51 train_loss : 3.384920597076416\n",
      "Epoch 52 train_loss : 3.277284860610962\n",
      "Epoch 53 train_loss : 3.2286550998687744\n",
      "Epoch 54 train_loss : 3.351900100708008\n",
      "Epoch 55 train_loss : 3.262019634246826\n",
      "Epoch 56 train_loss : 3.2704811096191406\n",
      "Epoch 57 train_loss : 3.366582155227661\n",
      "Epoch 58 train_loss : 3.2879936695098877\n",
      "Epoch 59 train_loss : 3.2397520542144775\n",
      "Epoch 60 train_loss : 3.144834041595459\n",
      "Epoch 61 train_loss : 3.167384147644043\n",
      "Epoch 62 train_loss : 3.170328378677368\n",
      "Epoch 63 train_loss : 3.0718109607696533\n",
      "Epoch 64 train_loss : 3.061962604522705\n",
      "Epoch 65 train_loss : 3.0884599685668945\n",
      "Epoch 66 train_loss : 3.1266443729400635\n",
      "Epoch 67 train_loss : 3.027689218521118\n",
      "Epoch 68 train_loss : 3.0570125579833984\n",
      "Epoch 69 train_loss : 3.0544350147247314\n",
      "Epoch 70 train_loss : 2.979938507080078\n",
      "Epoch 71 train_loss : 2.970054864883423\n",
      "Epoch 72 train_loss : 2.9982798099517822\n",
      "Epoch 73 train_loss : 2.9563565254211426\n",
      "Epoch 74 train_loss : 2.9870402812957764\n",
      "Epoch 75 train_loss : 2.933974027633667\n",
      "Epoch 76 train_loss : 2.8992726802825928\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_train_rmse, best_val_rmse, best_test_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR_bar_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/models/joint_model.py:283\u001b[0m, in \u001b[0;36mJointModel.fit\u001b[0;34m(self, R_bar, R_bar_val, R_bar_test, R_train, R_val, R_test)\u001b[0m\n\u001b[1;32m    280\u001b[0m patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep):\n\u001b[0;32m--> 283\u001b[0m     train_loss, val_loss, test_loss, ae_loss_X, ae_loss_Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer_M\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mae_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR_train_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR_val_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR_test_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m train_loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(train_loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m#print(f'Epoch {i+1} val_loss : {tf.math.sqrt(val_loss)}')\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m#print(f'Epoch {i+1} test_loss : {tf.math.sqrt(test_loss)}')\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# Early stopping check\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/models/joint_model.py:265\u001b[0m, in \u001b[0;36mJointModel.train_step\u001b[0;34m(self, optimizer_M, ae_optimizer, R_train, R_bar_train, R_train_mask, R_val, R_val_mask, R_test, R_test_mask)\u001b[0m\n\u001b[1;32m    262\u001b[0m ae_grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(total_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder_X\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder_Y\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m    263\u001b[0m ae_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(ae_grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder_X\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder_Y\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tape\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_rec, val_loss, test_loss, loss_autoencoder_X, loss_autoencoder_Y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_train_rmse, best_val_rmse, best_test_rmse = model.fit(R_bar_train, R_bar_val, R_bar_test, R_train, R_val, R_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.joint_model import JointModel\n",
    "\n",
    "# model = JointModel(config, dataset_name, 'Weighted_XMY')\n",
    "model = XMY(latent_side_info, latent_M, lambda_, 1000, dataset_name, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rmse : tf.Tensor(134.82095, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(25.278225, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(15.830902, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(11.510918, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(8.729328, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(7.0511446, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(7.0346913, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(9.223146, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(51.757206, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(4.8253813, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(6.5064387, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(7.334402, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(6.8647223, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(5.7995777, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(5.9679575, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(4.2167177, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(3.6763873, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(3.3763344, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.987894, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.5749583, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.6584187, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.3580503, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.5764265, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.4495707, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.4349048, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.2277486, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.0260403, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.0857663, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(2.0783334, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.9453155, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.7938715, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.699056, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.6690459, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.672108, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.6609216, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.6161621, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.5425675, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.4647366, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.4162487, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.400636, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.3930955, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.3742124, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.3445715, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.3166075, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.2953447, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.2796165, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.2629262, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.2422798, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.2221878, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.2053646, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.202465, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.180846, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.1698451, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.157407, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.144741, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.1369227, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.1311984, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.1269912, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.1211895, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.1132575, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.1113942, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0998719, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0940235, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0887932, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0839396, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0795778, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0757923, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0720962, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0687094, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0656751, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0627943, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.060131, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0575258, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0548261, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0523791, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0502867, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0483235, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0463971, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0444192, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0424448, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0406563, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0389736, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0372583, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0355862, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0340104, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0325297, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0311577, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0298259, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0285021, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0272682, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0260966, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0249645, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.023887, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0228286, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0218127, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0208372, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0198733, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0189437, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0180416, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0171665, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0163349, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.015514, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.014707, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0139251, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0131705, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0124456, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0117377, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0110483, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0103602, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0096837, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0090204, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0083892, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0077631, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0071467, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0065506, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0059679, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0053962, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0048261, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0042579, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0036922, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0031499, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.002993, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0021123, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0015856, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0010647, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0005487, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(1.0000236, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.9995024, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.9989715, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.99844825, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.9979239, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.9973951, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.9968656, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.9963262, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.995786, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.99523735, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.99468046, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.9941172, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.9935478, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.99297357, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.99239874, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.9918254, shape=(), dtype=float32)\n",
      "train_rmse : tf.Tensor(0.99126667, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_train_rmse, best_val_rmse, best_test_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR_bar_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 193\u001b[0m, in \u001b[0;36mXMY.fit\u001b[0;34m(self, R_bar_train, R_bar_val, R_bar_test, R_train, R_val, R_test, tensorboard_dir)\u001b[0m\n\u001b[1;32m    191\u001b[0m autoencoder_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder_X\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder_Y\u001b[38;5;241m.\u001b[39mtrainable_variables\n\u001b[1;32m    192\u001b[0m ae_grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(total_loss, autoencoder_variables)\n\u001b[0;32m--> 193\u001b[0m \u001b[43mae_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mae_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tape\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Compute rmse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:672\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    669\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_gradients(grads_and_vars)\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_utils\u001b[38;5;241m.\u001b[39mstrategy_supports_no_merge_call():\n\u001b[0;32m--> 672\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mapply_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m    676\u001b[0m       functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distributed_apply, apply_state\u001b[38;5;241m=\u001b[39mapply_state),\n\u001b[1;32m    677\u001b[0m       args\u001b[38;5;241m=\u001b[39m(grads_and_vars,),\n\u001b[1;32m    678\u001b[0m       kwargs\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    679\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: name,\n\u001b[1;32m    680\u001b[0m       })\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:721\u001b[0m, in \u001b[0;36mOptimizerV2._distributed_apply\u001b[0;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m distribution\u001b[38;5;241m.\u001b[39mextended\u001b[38;5;241m.\u001b[39mcolocate_vars_with(var):\n\u001b[1;32m    718\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m name_scope_only_in_function_or_graph(\n\u001b[1;32m    719\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eagerly_outside_functions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    720\u001b[0m       var\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mname):\n\u001b[0;32m--> 721\u001b[0m     update_op \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39min_cross_replica_context():\n\u001b[1;32m    724\u001b[0m       \u001b[38;5;66;03m# In cross-replica context, extended.update returns a list of\u001b[39;00m\n\u001b[1;32m    725\u001b[0m       \u001b[38;5;66;03m# update ops from all replicas (group=False).\u001b[39;00m\n\u001b[1;32m    726\u001b[0m       update_ops\u001b[38;5;241m.\u001b[39mextend(update_op)\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2634\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2631\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   2632\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2633\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2636\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2637\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3709\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   3707\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   3708\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 3709\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3715\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3711\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   3712\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   3713\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   3714\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 3715\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   3717\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:601\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    600\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:704\u001b[0m, in \u001b[0;36mOptimizerV2._distributed_apply.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_state\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_apply_args:\n\u001b[1;32m    703\u001b[0m   apply_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m apply_state\n\u001b[0;32m--> 704\u001b[0m update_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resource_apply_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mapply_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var\u001b[38;5;241m.\u001b[39mconstraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies([update_op]):\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:165\u001b[0m, in \u001b[0;36mAdam._resource_apply_dense\u001b[0;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[1;32m    162\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_slot(var, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamsgrad:\n\u001b[0;32m--> 165\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResourceApplyAdam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m      \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m      \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbeta1_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoefficients\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta_1_power\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbeta2_power\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoefficients\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta_2_power\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoefficients\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoefficients\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta_1_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoefficients\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta_2_t\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoefficients\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m      \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_locking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_locking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m   vhat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_slot(var, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvhat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/tensorflow/python/util/tf_export.py:404\u001b[0m, in \u001b[0;36mkwarg_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[1;32m    400\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{f}\u001b[39;00m\u001b[38;5;124m only takes keyword args (possible keys: \u001b[39m\u001b[38;5;132;01m{kwargs}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    402\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease pass these args as kwargs instead.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    403\u001b[0m       \u001b[38;5;241m.\u001b[39mformat(f\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, kwargs\u001b[38;5;241m=\u001b[39mf_argspec\u001b[38;5;241m.\u001b[39margs))\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/tensorflow/python/ops/gen_training_ops.py:1421\u001b[0m, in \u001b[0;36mresource_apply_adam\u001b[0;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   1420\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1421\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResourceApplyAdam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta1_power\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2_power\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_locking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_locking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_nesterov\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_nesterov\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   1426\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_train_rmse, best_val_rmse, best_test_rmse = model.fit(R_bar_train, R_bar_val, R_bar_test, R_train, R_val, R_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implicit_feedback_env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
