{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:59:36.698917Z",
     "iopub.status.busy": "2024-01-28T12:59:36.698338Z",
     "iopub.status.idle": "2024-01-28T12:59:39.068837Z",
     "shell.execute_reply": "2024-01-28T12:59:39.068222Z",
     "shell.execute_reply.started": "2024-01-28T12:59:36.698888Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T12:59:39.996025Z",
     "iopub.status.busy": "2024-01-28T12:59:39.995414Z",
     "iopub.status.idle": "2024-01-28T12:59:41.614384Z",
     "shell.execute_reply": "2024-01-28T12:59:41.613733Z",
     "shell.execute_reply.started": "2024-01-28T12:59:39.995990Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = 'ml_100k'\n",
    "p_value = 0.05\n",
    "\n",
    "R_bar_train, R_bar_val, R_bar_test, R_train, R_val, R_test = prepare_data(dataset_name, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-28T13:00:21.074176Z",
     "iopub.status.busy": "2024-01-28T13:00:21.073849Z",
     "iopub.status.idle": "2024-01-28T13:00:26.173376Z",
     "shell.execute_reply": "2024-01-28T13:00:26.172761Z",
     "shell.execute_reply.started": "2024-01-28T13:00:21.074141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 - 3s - loss: 2369704.2500 - val_loss: 244157.0000 - 3s/epoch - 95ms/step\n",
      "Epoch 2/10\n",
      "30/30 - 0s - loss: 2077906.0000 - val_loss: 11678207.0000 - 153ms/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "30/30 - 0s - loss: 1500731.8750 - val_loss: 2853003.7500 - 159ms/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "30/30 - 0s - loss: 769324.1250 - val_loss: 237282.0625 - 171ms/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "30/30 - 0s - loss: 253514.1562 - val_loss: 1976791.5000 - 211ms/epoch - 7ms/step\n",
      "Epoch 6/10\n",
      "30/30 - 0s - loss: 55121.4258 - val_loss: 146477.1094 - 204ms/epoch - 7ms/step\n",
      "Epoch 7/10\n",
      "30/30 - 0s - loss: 12280.3496 - val_loss: 266701.9688 - 158ms/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "30/30 - 0s - loss: 4876.9175 - val_loss: 1069914.5000 - 170ms/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "30/30 - 0s - loss: 4209.5083 - val_loss: 14872.2314 - 174ms/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "30/30 - 0s - loss: 5225.8154 - val_loss: 2090.9375 - 181ms/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d8c977b20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from utils.weights_utils import MatrixProcessor\n",
    "from autoencoders.weighted_mse_autoencoder import UserAutoEncoder, ItemAutoEncoder\n",
    "\n",
    "matrix_processor = MatrixProcessor()\n",
    "user_matrix_multiplier = matrix_processor.compute_terms(R_bar_train)\n",
    "\n",
    "batch_size = 32\n",
    "user_dataset = tf.data.Dataset.from_tensor_slices((user_matrix_multiplier, user_matrix_multiplier))\n",
    "user_dataset = user_dataset.shuffle(buffer_size=user_matrix_multiplier.shape[0]).batch(batch_size)\n",
    "\n",
    "ae_optimizer = Adam(learning_rate=0.01)\n",
    "\n",
    "autoencoder_X = UserAutoEncoder(user_matrix_multiplier.shape[1], 4)\n",
    "autoencoder_X.compile(optimizer=ae_optimizer, loss='mse')\n",
    "\n",
    "autoencoder_X.fit(user_dataset, validation_data=user_dataset, epochs=10, verbose=2, batch_size=batch_size, callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munchong/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "2023-12-24 15:11:51.157949: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from utils.data_utils import preprocess_data\n",
    "\n",
    "dataset = 'ml_100k'\n",
    "p_value = 0.1\n",
    "# Prepare dataset\n",
    "R_bar_train, R_bar_val, _, _, _, _ = preprocess_data(dataset=dataset, p=p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: (10, 3)\n",
      "Target tensor shape: (10, 3)\n",
      "Weights tensor shape: (10, 3)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Example dummy data\n",
    "# dummy_inputs = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], dtype=np.float32)  # Shape (2, 3)\n",
    "# dummy_targets = np.array([[0.2, 0.3, 0.4], [0.5, 0.6, 0.7]], dtype=np.float32)  # Shape (2, 3)\n",
    "# dummy_weights = np.array([[1, 2, 1], [2, 1, 2]], dtype=np.float32)  # Shape (2, 3)\n",
    "\n",
    "# # Convert dummy data to tensors if they are not already\n",
    "# dummy_inputs = tf.convert_to_tensor(dummy_inputs)\n",
    "# dummy_targets = tf.convert_to_tensor(dummy_targets)\n",
    "# dummy_weights = tf.convert_to_tensor(dummy_weights)\n",
    "\n",
    "# # Check shapes\n",
    "# print(\"Input shape:\", dummy_inputs.shape)\n",
    "# print(\"Target shape:\", dummy_targets.shape)\n",
    "# print(\"Weights shape:\", dummy_weights.shape)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generate 10 dummy rows of data\n",
    "dummy_inputs = np.random.rand(10, 3).astype(np.float32)  # Shape (10, 3)\n",
    "#dummy_targets = np.random.rand(10, 3).astype(np.float32)  # Shape (10, 3)\n",
    "dummy_weights = np.random.randint(1, 5, (10, 3)).astype(np.float32)  # Shape (10, 3)\n",
    "\n",
    "# Convert dummy data to tensors\n",
    "dummy_inputs_tensor = tf.convert_to_tensor(dummy_inputs)\n",
    "dummy_targets_tensor = tf.convert_to_tensor(dummy_inputs)\n",
    "dummy_weights_tensor = tf.convert_to_tensor(dummy_weights)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Input tensor shape:\", dummy_inputs_tensor.shape)\n",
    "print(\"Target tensor shape:\", dummy_targets_tensor.shape)\n",
    "print(\"Weights tensor shape:\", dummy_weights_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02102008, 0.22426595, 0.38649988],\n",
       "       [0.6241214 , 0.7362278 , 0.7012032 ],\n",
       "       [0.22834125, 0.28006727, 0.95931756],\n",
       "       [0.3313171 , 0.7340466 , 0.4501734 ],\n",
       "       [0.87129796, 0.8634871 , 0.81843334],\n",
       "       [0.6178341 , 0.6822416 , 0.01459227],\n",
       "       [0.94953674, 0.43377674, 0.13697594],\n",
       "       [0.790288  , 0.0448177 , 0.7756005 ],\n",
       "       [0.20874563, 0.9547339 , 0.3719767 ],\n",
       "       [0.9862683 , 0.84811366, 0.74733883]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 3), dtype=float32, numpy=\n",
       "array([[0.02102008, 0.22426595, 0.38649988],\n",
       "       [0.6241214 , 0.7362278 , 0.7012032 ],\n",
       "       [0.22834125, 0.28006727, 0.95931756],\n",
       "       [0.3313171 , 0.7340466 , 0.4501734 ],\n",
       "       [0.87129796, 0.8634871 , 0.81843334],\n",
       "       [0.6178341 , 0.6822416 , 0.01459227],\n",
       "       [0.94953674, 0.43377674, 0.13697594],\n",
       "       [0.790288  , 0.0448177 , 0.7756005 ],\n",
       "       [0.20874563, 0.9547339 , 0.3719767 ],\n",
       "       [0.9862683 , 0.84811366, 0.74733883]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 1., 1.],\n",
       "       [1., 4., 4.],\n",
       "       [2., 3., 1.],\n",
       "       [3., 3., 1.],\n",
       "       [2., 4., 2.],\n",
       "       [3., 4., 4.],\n",
       "       [2., 3., 3.],\n",
       "       [1., 2., 2.],\n",
       "       [1., 4., 3.],\n",
       "       [1., 4., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def data_generator(inputs, targets, weights, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets, weights))\n",
    "    dataset = dataset.shuffle(buffer_size=len(inputs)).batch(batch_size)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_generator(dummy_inputs_tensor, dummy_targets_tensor, dummy_weights_tensor, batch_size=2)\n",
    "val_dataset = data_generator(R_bar_val, R_bar_val, weights, batch_size=64)  # If you have separate validation weights, use those instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Inputs: [[0.22834125 0.28006727 0.95931756]\n",
      " [0.94953674 0.43377674 0.13697594]]\n",
      "Targets: [[0.22834125 0.28006727 0.95931756]\n",
      " [0.94953674 0.43377674 0.13697594]]\n",
      "Weights: [[4. 4. 2.]\n",
      " [4. 2. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the train_dataset\n",
    "for batch, (inputs, targets, batch_weights) in enumerate(train_dataset):\n",
    "    print(f\"Batch {batch}\")\n",
    "    print(\"Inputs:\", inputs.numpy())\n",
    "    print(\"Targets:\", targets.numpy())\n",
    "    print(\"Weights:\", batch_weights.numpy())\n",
    "    # Break after the first iteration for demonstration purposes\n",
    "    # Remove the following line to iterate over the whole dataset\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Weighted Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from scipy.linalg import svd\n",
    "\n",
    "class MatrixProcessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_probabilities(matrix):\n",
    "        # Sum the entries in each row\n",
    "        row_sums = tf.reduce_sum(matrix, axis=1)\n",
    "\n",
    "        # Total sum of entries in the matrix\n",
    "        total_entries = tf.reduce_sum(matrix)\n",
    "\n",
    "        # Compute empirical probabilities\n",
    "        p_hat = row_sums / total_entries\n",
    "\n",
    "        # Compute smoothed probabilities\n",
    "        m = matrix.shape[0]  # Number of rows in the matrix\n",
    "        p_check = 0.5 * (p_hat + (1/m))\n",
    "\n",
    "        return p_hat, p_check\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_terms(matrix):\n",
    "        # Compute the empirical and smoothed probabilities\n",
    "        p_hat, p_check = MatrixProcessor.compute_probabilities(matrix)\n",
    "        q_hat, q_check = MatrixProcessor.compute_probabilities(tf.transpose(matrix))\n",
    "\n",
    "        # Convert the probabilities to diagonal matrices\n",
    "#         diagonal_P = tf.cast(tf.linalg.diag(p_hat), dtype=tf.float32)\n",
    "#         diagonal_Q = tf.cast(tf.linalg.diag(q_hat), dtype=tf.float32)\n",
    "#         P_check = tf.cast(tf.linalg.diag(p_check), dtype=tf.float32)\n",
    "#         Q_check = tf.cast(tf.linalg.diag(q_check), dtype=tf.float32)\n",
    "\n",
    "        # Compute new matrix\n",
    "        p_check_matrix = tf.tile(tf.expand_dims(p_check, 1), [1, len(q_check)])\n",
    "        q_check_matrix = tf.tile(tf.expand_dims(q_check, 0), [len(p_check), 1])\n",
    "        new_matrix = tf.math.rsqrt(p_check_matrix * q_check_matrix)\n",
    "\n",
    "        return new_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_svd(matrix, latent_dim):\n",
    "        # Compute the SVD and save the first k singular vectors/values\n",
    "        U, s, Vt = svd(matrix, full_matrices=False)\n",
    "\n",
    "        X = tf.cast(U[:, :latent_dim], dtype=tf.float32)\n",
    "        Y = tf.cast(Vt[:latent_dim, :].T, dtype=tf.float32)\n",
    "\n",
    "        return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = MatrixProcessor.compute_terms(R_bar_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Autoencoder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munchong/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "\n",
    "def reset_random_seeds():\n",
    "   os.environ['PYTHONHASHSEED']=str(1)\n",
    "   tf.random.set_seed(1)\n",
    "   np.random.seed(1)\n",
    "   random.seed(1)\n",
    "\n",
    "reset_random_seeds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 14:20:24.985754: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from autoencoders.weighted_autoencoder import WeightedUserAutoEncoder, WeightedItemAutoEncoder, CustomLoss\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example dummy data\n",
    "num_samples = 5\n",
    "input_feature_len = 2  # Adjust as per your actual feature length\n",
    "\n",
    "# Create dummy data\n",
    "inputs = np.random.random((num_samples, input_feature_len))\n",
    "targets = np.random.random((num_samples, input_feature_len))\n",
    "weights = np.random.random((num_samples, input_feature_len))\n",
    "\n",
    "batch_size = 5  # Adjust batch size as needed\n",
    "\n",
    "def data_generator(inputs, targets, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "    dataset = dataset.shuffle(buffer_size=len(inputs)).batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = data_generator(inputs, inputs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_bar_train_weighted = MatrixProcessor.compute_terms(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = data_generator(R_bar_train_weighted, R_bar_train_weighted, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "# # Set seeds\n",
    "# seed_value = 42\n",
    "# tf.random.set_seed(seed_value)\n",
    "# np.random.seed(seed_value)\n",
    "# random.seed(seed_value)\n",
    "class LossHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print('Logs :', logs)\n",
    "        #print(f\"Training: Batch {batch}, Loss {logs['batch_loss']}\")\n",
    "\n",
    "autoencoder = WeightedUserAutoEncoder(input_feature_len, 2, [2], [input_feature_len], 0.001, 0.1, False)\n",
    "autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.17022005e-01, 7.20324493e-01],\n",
       "       [1.14374817e-04, 3.02332573e-01],\n",
       "       [1.46755891e-01, 9.23385948e-02],\n",
       "       [1.86260211e-01, 3.45560727e-01],\n",
       "       [3.96767474e-01, 5.38816734e-01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Logs : {'mse': 0.15510718524456024}\n",
      "1/1 - 0s - mse: 0.1551 - val_loss: 0.1697 - val_mse: 0.1649 - 405ms/epoch - 405ms/step\n",
      "Epoch 2/2\n",
      "Logs : {'mse': 0.16879650950431824}\n",
      "1/1 - 0s - mse: 0.1688 - val_loss: 0.1677 - val_mse: 0.1629 - 10ms/epoch - 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137977250>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(train_dataset, \n",
    "                epochs=2, \n",
    "                # batch_size=128,\n",
    "                validation_data=train_dataset, \n",
    "                verbose=2,\n",
    "                shuffle=True,\n",
    "                callbacks=[LossHistory()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "X batch train : tf.Tensor(\n",
      "[[1.46755891e-01 9.23385948e-02]\n",
      " [4.17022005e-01 7.20324493e-01]\n",
      " [1.14374817e-04 3.02332573e-01]\n",
      " [1.86260211e-01 3.45560727e-01]\n",
      " [3.96767474e-01 5.38816734e-01]], shape=(5, 2), dtype=float64)\n",
      "y batch train : tf.Tensor(\n",
      "[[1.46755889e-01 9.23385918e-02]\n",
      " [4.17021990e-01 7.20324516e-01]\n",
      " [1.14374816e-04 3.02332580e-01]\n",
      " [1.86260208e-01 3.45560730e-01]\n",
      " [3.96767467e-01 5.38816750e-01]], shape=(5, 2), dtype=float32)\n",
      "predictions : tf.Tensor(\n",
      "[[-0.06717951  0.01890692]\n",
      " [-0.278314    0.03892186]\n",
      " [-0.05775178 -0.00975677]\n",
      " [-0.04371599 -0.02262597]\n",
      " [-0.23682646  0.0417682 ]], shape=(5, 2), dtype=float32)\n",
      "Batch 0, Loss: 0.19366595149040222\n",
      "Average Loss for Epoch 1: 0.19366595149040222\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "epochs = 1\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "\n",
    "reset_random_seeds()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = autoencoder(x_batch_train, training=True)\n",
    "\n",
    "            # Ensure y_batch_train is of the same type as predictions\n",
    "            y_batch_train = tf.cast(y_batch_train, tf.float32)\n",
    "\n",
    "            # Custom computation of squared error\n",
    "            squared_difference = tf.square(y_batch_train - predictions)\n",
    "            # Compute the batch loss\n",
    "            batch_loss = tf.reduce_sum(squared_difference) / tf.cast(tf.size(y_batch_train), dtype=tf.float32)\n",
    "\n",
    "        #grads = tape.gradient(batch_loss, autoencoder.trainable_variables)\n",
    "        #autoencoder.optimizer.apply_gradients(zip(grads, autoencoder.trainable_variables))\n",
    "\n",
    "        print('X batch train :', x_batch_train)\n",
    "        print('y batch train :', y_batch_train)\n",
    "        print('predictions :', predictions)\n",
    "        print(f\"Batch {step}, Loss: {batch_loss.numpy()}\")\n",
    "        # Accumulate average batch loss\n",
    "        total_loss += batch_loss\n",
    "        num_batches += 1\n",
    "\n",
    "    # Compute average loss over all batches\n",
    "    avg_loss_per_epoch = total_loss / num_batches\n",
    "    print(f\"Average Loss for Epoch {epoch + 1}: {avg_loss_per_epoch.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6165734999999999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([[-0.06717951,  0.01890692],\n",
    " [-0.278314,    0.03892186],\n",
    " [-0.05775178, -0.00975677],\n",
    " [-0.04371599, -0.02262597],\n",
    " [-0.23682646,  0.0417682 ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
