{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders.autoencoder import UserAutoEncoder, ItemAutoEncoder\n",
    "from utils.data_utils import preprocess_data\n",
    "from utils.evaluation import calculate_recall_at_k\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'ml_100k'\n",
    "p_value = 0.1\n",
    "R_bar_train, R_bar_val, _, _, _, _ = preprocess_data(dataset=dataset ,p=p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "class WandbLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            wandb.log({'loss': logs['loss'], 'val_loss': logs['val_loss']})\n",
    "\n",
    "# def wandb_callback(study, trial):\n",
    "#     # Log trial metrics and parameters to wandb\n",
    "#     wandb.log({\n",
    "#         'trial_number': trial.number,\n",
    "#         'trial_value': trial.value,\n",
    "#         'parameters': trial.params\n",
    "#     })\n",
    "\n",
    "def objective(trial, dataset, p_value, model_type, R_bar_train, R_bar_val):\n",
    "    # Shared hyperparameters\n",
    "    latent_dim = trial.suggest_categorical('latent_dim', [2, 4, 8, 16, 32, 64, 128, 256])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    regularizer_factor = trial.suggest_categorical('regularizer_factor', [0, 1e-2])\n",
    "    dropout_rate = trial.suggest_categorical('dropout_rate', [0.0, 0.1, 0.2, 0.3])\n",
    "    use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
    "\n",
    "    # Create a unique run name using the trial number and hyperparameters\n",
    "    run_name = f\"dataset_{dataset}, trial_{trial.number}, latent_{latent_dim}, layers_{num_layers}, regularizer_factor_{regularizer_factor}, dropout_rate_{dropout_rate}, use_batch_norm_{use_batch_norm}\"\n",
    "    \n",
    "    # Initialize a new wandb run for each trial\n",
    "    wandb.init(project=\"autoencoder_optimization\", name=run_name, reinit=True, \n",
    "               config={\n",
    "                   'dataset': dataset,\n",
    "                   'p_value': p_value,\n",
    "                   'latent_dim': latent_dim,\n",
    "                   'num_layers': num_layers,\n",
    "                   'regularizer_factor': regularizer_factor,\n",
    "                   'dropout_rate': dropout_rate,\n",
    "                   'use_batch_norm': use_batch_norm\n",
    "               })\n",
    "\n",
    "    # Initialize the model based on the chosen type\n",
    "    if model_type == \"user\":\n",
    "        autoencoder = UserAutoEncoder(R_bar_train.shape[1], latent_dim, num_layers, regularizer_factor, dropout_rate, use_batch_norm)\n",
    "    elif model_type == \"item\":\n",
    "        autoencoder = ItemAutoEncoder(R_bar_train.shape[0], latent_dim, num_layers, regularizer_factor, dropout_rate, use_batch_norm)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type specified\")\n",
    "\n",
    "    # Set up EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "    # Compile and train the model with the WandbLogger callback\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    autoencoder.fit(R_bar_train, R_bar_train, \n",
    "                    epochs=1000, \n",
    "                    batch_size=256,\n",
    "                    validation_data=(R_bar_val, R_bar_val), \n",
    "                    verbose=2, \n",
    "                    callbacks=[WandbLogger(), early_stopping])\n",
    "\n",
    "    # Generate recommendations and compute recall_at_k\n",
    "    # Note: Modify this to suit how you generate predictions and evaluate recall for your model\n",
    "    pred_val = autoencoder.predict(R_bar_val)\n",
    "    \n",
    "    k_values = [5, 10, 50, 100]\n",
    "    avg_recall_at_k, avg_random_recall_at_k = calculate_recall_at_k(R_bar_train, R_bar_val, pred_val, k_values)\n",
    "\n",
    "    # Log each recall_at_k and random_recall_at_k value as separate columns\n",
    "    for k in k_values:\n",
    "        wandb.log({f'recall_at_{k}': avg_recall_at_k[k], f'random_recall_at_{k}': avg_random_recall_at_k[k]})\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    return -avg_recall_at_k[10]  # Optimize for a specific recall value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 19:54:55,513] A new study created in memory with name: no-name-fe3c2021-fea1-4408-bd08-03f3591b1627\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/munchong/Desktop/recsys/implicit_feedback_recsys/wandb/run-20231216_195455-8ejqhifn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/munchong915/autoencoder_optimization/runs/8ejqhifn' target=\"_blank\">dataset_ml_100k, trial_0, latent_8, layers_2, regularizer_factor_0.01, dropout_rate_0.1, use_batch_norm_False</a></strong> to <a href='https://wandb.ai/munchong915/autoencoder_optimization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/munchong915/autoencoder_optimization' target=\"_blank\">https://wandb.ai/munchong915/autoencoder_optimization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/munchong915/autoencoder_optimization/runs/8ejqhifn' target=\"_blank\">https://wandb.ai/munchong915/autoencoder_optimization/runs/8ejqhifn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "4/4 - 2s - loss: 6.6627 - val_loss: 5.5493 - 2s/epoch - 575ms/step\n",
      "Epoch 2/1000\n",
      "4/4 - 1s - loss: 4.9958 - val_loss: 4.2379 - 975ms/epoch - 244ms/step\n",
      "Epoch 3/1000\n",
      "4/4 - 1s - loss: 3.7705 - val_loss: 3.2756 - 1s/epoch - 251ms/step\n",
      "Epoch 4/1000\n",
      "4/4 - 1s - loss: 3.0114 - val_loss: 2.6968 - 1s/epoch - 252ms/step\n",
      "Epoch 5/1000\n",
      "4/4 - 1s - loss: 2.4996 - val_loss: 2.2854 - 987ms/epoch - 247ms/step\n",
      "Epoch 6/1000\n",
      "4/4 - 2s - loss: 2.1495 - val_loss: 1.9290 - 2s/epoch - 382ms/step\n",
      "Epoch 7/1000\n",
      "4/4 - 1s - loss: 1.8838 - val_loss: 1.6627 - 1s/epoch - 298ms/step\n",
      "Epoch 8/1000\n",
      "4/4 - 1s - loss: 1.6674 - val_loss: 1.4595 - 1s/epoch - 308ms/step\n",
      "Epoch 9/1000\n",
      "4/4 - 1s - loss: 1.4799 - val_loss: 1.2850 - 1s/epoch - 372ms/step\n",
      "Epoch 10/1000\n",
      "4/4 - 1s - loss: 1.3193 - val_loss: 1.1284 - 1s/epoch - 349ms/step\n",
      "Epoch 11/1000\n",
      "4/4 - 1s - loss: 1.1812 - val_loss: 0.9991 - 1s/epoch - 286ms/step\n",
      "Epoch 12/1000\n",
      "4/4 - 1s - loss: 1.0664 - val_loss: 0.8975 - 1s/epoch - 304ms/step\n",
      "Epoch 13/1000\n",
      "4/4 - 1s - loss: 0.9714 - val_loss: 0.8161 - 1s/epoch - 291ms/step\n",
      "Epoch 14/1000\n",
      "4/4 - 1s - loss: 0.8920 - val_loss: 0.7473 - 1s/epoch - 269ms/step\n",
      "Epoch 15/1000\n",
      "4/4 - 1s - loss: 0.8245 - val_loss: 0.6851 - 1s/epoch - 273ms/step\n",
      "Epoch 16/1000\n",
      "4/4 - 1s - loss: 0.7656 - val_loss: 0.6298 - 1s/epoch - 258ms/step\n",
      "Epoch 17/1000\n",
      "4/4 - 1s - loss: 0.7137 - val_loss: 0.5805 - 1s/epoch - 259ms/step\n",
      "Epoch 18/1000\n",
      "4/4 - 1s - loss: 0.6673 - val_loss: 0.5371 - 1s/epoch - 285ms/step\n",
      "Epoch 19/1000\n",
      "4/4 - 1s - loss: 0.6257 - val_loss: 0.4982 - 1s/epoch - 253ms/step\n",
      "Epoch 20/1000\n",
      "4/4 - 1s - loss: 0.5880 - val_loss: 0.4636 - 1s/epoch - 259ms/step\n",
      "Epoch 21/1000\n",
      "4/4 - 1s - loss: 0.5537 - val_loss: 0.4310 - 1s/epoch - 267ms/step\n",
      "Epoch 22/1000\n",
      "4/4 - 1s - loss: 0.5221 - val_loss: 0.3998 - 1s/epoch - 251ms/step\n",
      "Epoch 23/1000\n",
      "4/4 - 1s - loss: 0.4926 - val_loss: 0.3716 - 996ms/epoch - 249ms/step\n",
      "Epoch 24/1000\n",
      "4/4 - 1s - loss: 0.4651 - val_loss: 0.3441 - 982ms/epoch - 246ms/step\n",
      "Epoch 25/1000\n",
      "4/4 - 1s - loss: 0.4396 - val_loss: 0.3170 - 1s/epoch - 252ms/step\n",
      "Epoch 26/1000\n",
      "4/4 - 1s - loss: 0.4159 - val_loss: 0.2948 - 1s/epoch - 265ms/step\n",
      "Epoch 27/1000\n",
      "4/4 - 1s - loss: 0.3940 - val_loss: 0.2712 - 996ms/epoch - 249ms/step\n",
      "Epoch 28/1000\n",
      "4/4 - 1s - loss: 0.3740 - val_loss: 0.2534 - 1s/epoch - 251ms/step\n",
      "Epoch 29/1000\n",
      "4/4 - 1s - loss: 0.3555 - val_loss: 0.2354 - 1s/epoch - 257ms/step\n",
      "Epoch 30/1000\n",
      "4/4 - 1s - loss: 0.3384 - val_loss: 0.2179 - 1s/epoch - 252ms/step\n",
      "Epoch 31/1000\n",
      "4/4 - 1s - loss: 0.3227 - val_loss: 0.2042 - 1s/epoch - 277ms/step\n",
      "Epoch 32/1000\n",
      "4/4 - 1s - loss: 0.3087 - val_loss: 0.1908 - 1s/epoch - 276ms/step\n",
      "Epoch 33/1000\n",
      "4/4 - 1s - loss: 0.2957 - val_loss: 0.1789 - 1s/epoch - 253ms/step\n",
      "Epoch 34/1000\n",
      "4/4 - 1s - loss: 0.2842 - val_loss: 0.1657 - 1s/epoch - 255ms/step\n",
      "Epoch 35/1000\n",
      "4/4 - 1s - loss: 0.2735 - val_loss: 0.1570 - 1s/epoch - 255ms/step\n",
      "Epoch 36/1000\n",
      "4/4 - 1s - loss: 0.2632 - val_loss: 0.1490 - 1s/epoch - 255ms/step\n",
      "Epoch 37/1000\n",
      "4/4 - 1s - loss: 0.2545 - val_loss: 0.1398 - 1s/epoch - 252ms/step\n",
      "Epoch 38/1000\n",
      "4/4 - 1s - loss: 0.2460 - val_loss: 0.1313 - 1s/epoch - 255ms/step\n",
      "Epoch 39/1000\n",
      "4/4 - 1s - loss: 0.2380 - val_loss: 0.1228 - 1s/epoch - 282ms/step\n",
      "Epoch 40/1000\n",
      "4/4 - 1s - loss: 0.2311 - val_loss: 0.1175 - 1s/epoch - 279ms/step\n",
      "Epoch 41/1000\n",
      "4/4 - 1s - loss: 0.2245 - val_loss: 0.1123 - 1s/epoch - 251ms/step\n",
      "Epoch 42/1000\n",
      "4/4 - 1s - loss: 0.2187 - val_loss: 0.1051 - 1s/epoch - 260ms/step\n",
      "Epoch 43/1000\n",
      "4/4 - 1s - loss: 0.2132 - val_loss: 0.1005 - 1s/epoch - 252ms/step\n",
      "Epoch 44/1000\n",
      "4/4 - 1s - loss: 0.2084 - val_loss: 0.0968 - 1s/epoch - 267ms/step\n",
      "Epoch 45/1000\n",
      "4/4 - 1s - loss: 0.2040 - val_loss: 0.0923 - 996ms/epoch - 249ms/step\n",
      "Epoch 46/1000\n",
      "4/4 - 1s - loss: 0.1998 - val_loss: 0.0892 - 1s/epoch - 251ms/step\n",
      "Epoch 47/1000\n",
      "4/4 - 1s - loss: 0.1958 - val_loss: 0.0852 - 1s/epoch - 255ms/step\n",
      "Epoch 48/1000\n",
      "4/4 - 1s - loss: 0.1924 - val_loss: 0.0820 - 1s/epoch - 255ms/step\n",
      "Epoch 49/1000\n",
      "4/4 - 1s - loss: 0.1893 - val_loss: 0.0792 - 1s/epoch - 253ms/step\n",
      "Epoch 50/1000\n",
      "4/4 - 1s - loss: 0.1866 - val_loss: 0.0763 - 1s/epoch - 252ms/step\n",
      "Epoch 51/1000\n",
      "4/4 - 1s - loss: 0.1840 - val_loss: 0.0742 - 992ms/epoch - 248ms/step\n",
      "Epoch 52/1000\n",
      "4/4 - 1s - loss: 0.1816 - val_loss: 0.0712 - 1s/epoch - 251ms/step\n",
      "Epoch 53/1000\n",
      "4/4 - 1s - loss: 0.1796 - val_loss: 0.0699 - 1s/epoch - 254ms/step\n",
      "Epoch 54/1000\n",
      "4/4 - 1s - loss: 0.1776 - val_loss: 0.0679 - 1s/epoch - 251ms/step\n",
      "Epoch 55/1000\n",
      "4/4 - 1s - loss: 0.1759 - val_loss: 0.0660 - 978ms/epoch - 244ms/step\n",
      "Epoch 56/1000\n",
      "4/4 - 1s - loss: 0.1744 - val_loss: 0.0658 - 992ms/epoch - 248ms/step\n",
      "Epoch 57/1000\n",
      "4/4 - 1s - loss: 0.1732 - val_loss: 0.0636 - 1s/epoch - 252ms/step\n",
      "Epoch 58/1000\n",
      "4/4 - 1s - loss: 0.1714 - val_loss: 0.0624 - 983ms/epoch - 246ms/step\n",
      "Epoch 59/1000\n",
      "4/4 - 1s - loss: 0.1704 - val_loss: 0.0614 - 989ms/epoch - 247ms/step\n",
      "Epoch 60/1000\n",
      "4/4 - 1s - loss: 0.1694 - val_loss: 0.0603 - 1s/epoch - 263ms/step\n",
      "Epoch 61/1000\n",
      "4/4 - 1s - loss: 0.1683 - val_loss: 0.0590 - 979ms/epoch - 245ms/step\n",
      "Epoch 62/1000\n",
      "4/4 - 1s - loss: 0.1673 - val_loss: 0.0573 - 1s/epoch - 251ms/step\n",
      "Epoch 63/1000\n",
      "4/4 - 1s - loss: 0.1669 - val_loss: 0.0566 - 998ms/epoch - 250ms/step\n",
      "Epoch 64/1000\n",
      "4/4 - 1s - loss: 0.1664 - val_loss: 0.0547 - 1s/epoch - 251ms/step\n",
      "Epoch 65/1000\n",
      "4/4 - 1s - loss: 0.1665 - val_loss: 0.0535 - 1s/epoch - 252ms/step\n",
      "Epoch 66/1000\n",
      "4/4 - 1s - loss: 0.1663 - val_loss: 0.0540 - 1s/epoch - 264ms/step\n",
      "Epoch 67/1000\n",
      "4/4 - 1s - loss: 0.1653 - val_loss: 0.0565 - 1s/epoch - 255ms/step\n",
      "Epoch 68/1000\n",
      "4/4 - 1s - loss: 0.1645 - val_loss: 0.0581 - 1s/epoch - 286ms/step\n",
      "Epoch 69/1000\n",
      "4/4 - 1s - loss: 0.1641 - val_loss: 0.0551 - 1s/epoch - 257ms/step\n",
      "Epoch 70/1000\n",
      "4/4 - 1s - loss: 0.1635 - val_loss: 0.0521 - 1s/epoch - 257ms/step\n",
      "Epoch 71/1000\n",
      "4/4 - 1s - loss: 0.1635 - val_loss: 0.0523 - 1s/epoch - 287ms/step\n",
      "Epoch 72/1000\n",
      "4/4 - 1s - loss: 0.1628 - val_loss: 0.0534 - 1s/epoch - 258ms/step\n",
      "Epoch 73/1000\n",
      "4/4 - 1s - loss: 0.1621 - val_loss: 0.0544 - 1s/epoch - 305ms/step\n",
      "Epoch 74/1000\n",
      "4/4 - 1s - loss: 0.1619 - val_loss: 0.0534 - 1s/epoch - 277ms/step\n",
      "Epoch 75/1000\n",
      "4/4 - 1s - loss: 0.1613 - val_loss: 0.0522 - 974ms/epoch - 243ms/step\n",
      "Epoch 76/1000\n",
      "4/4 - 1s - loss: 0.1612 - val_loss: 0.0507 - 1s/epoch - 250ms/step\n",
      "Epoch 77/1000\n",
      "4/4 - 1s - loss: 0.1610 - val_loss: 0.0532 - 930ms/epoch - 233ms/step\n",
      "Epoch 78/1000\n",
      "4/4 - 1s - loss: 0.1609 - val_loss: 0.0530 - 948ms/epoch - 237ms/step\n",
      "Epoch 79/1000\n",
      "4/4 - 1s - loss: 0.1605 - val_loss: 0.0521 - 939ms/epoch - 235ms/step\n",
      "Epoch 80/1000\n",
      "4/4 - 1s - loss: 0.1604 - val_loss: 0.0511 - 928ms/epoch - 232ms/step\n",
      "Epoch 81/1000\n",
      "4/4 - 1s - loss: 0.1603 - val_loss: 0.0512 - 943ms/epoch - 236ms/step\n",
      "Epoch 82/1000\n",
      "4/4 - 1s - loss: 0.1601 - val_loss: 0.0499 - 1s/epoch - 299ms/step\n",
      "Epoch 83/1000\n",
      "4/4 - 1s - loss: 0.1600 - val_loss: 0.0517 - 1s/epoch - 295ms/step\n",
      "Epoch 84/1000\n",
      "4/4 - 1s - loss: 0.1597 - val_loss: 0.0512 - 1s/epoch - 282ms/step\n",
      "Epoch 85/1000\n",
      "4/4 - 1s - loss: 0.1598 - val_loss: 0.0517 - 1s/epoch - 287ms/step\n",
      "Epoch 86/1000\n",
      "4/4 - 1s - loss: 0.1596 - val_loss: 0.0501 - 1s/epoch - 307ms/step\n",
      "Epoch 87/1000\n",
      "4/4 - 1s - loss: 0.1597 - val_loss: 0.0499 - 1s/epoch - 335ms/step\n",
      "Epoch 88/1000\n",
      "4/4 - 2s - loss: 0.1596 - val_loss: 0.0495 - 2s/epoch - 397ms/step\n",
      "Epoch 89/1000\n",
      "4/4 - 2s - loss: 0.1599 - val_loss: 0.0489 - 2s/epoch - 385ms/step\n",
      "Epoch 90/1000\n",
      "4/4 - 1s - loss: 0.1599 - val_loss: 0.0492 - 1s/epoch - 308ms/step\n",
      "Epoch 91/1000\n",
      "4/4 - 1s - loss: 0.1597 - val_loss: 0.0490 - 1s/epoch - 299ms/step\n",
      "Epoch 92/1000\n",
      "4/4 - 1s - loss: 0.1596 - val_loss: 0.0498 - 1s/epoch - 289ms/step\n",
      "Epoch 93/1000\n",
      "4/4 - 1s - loss: 0.1594 - val_loss: 0.0519 - 1s/epoch - 282ms/step\n",
      "Epoch 94/1000\n",
      "4/4 - 1s - loss: 0.1593 - val_loss: 0.0513 - 1s/epoch - 305ms/step\n",
      "Epoch 95/1000\n",
      "4/4 - 1s - loss: 0.1590 - val_loss: 0.0512 - 1s/epoch - 287ms/step\n",
      "Epoch 96/1000\n",
      "4/4 - 1s - loss: 0.1590 - val_loss: 0.0505 - 1s/epoch - 263ms/step\n",
      "Epoch 97/1000\n",
      "4/4 - 1s - loss: 0.1587 - val_loss: 0.0509 - 1s/epoch - 266ms/step\n",
      "Epoch 98/1000\n",
      "4/4 - 1s - loss: 0.1590 - val_loss: 0.0510 - 992ms/epoch - 248ms/step\n",
      "Epoch 99/1000\n",
      "4/4 - 1s - loss: 0.1587 - val_loss: 0.0506 - 1s/epoch - 290ms/step\n",
      "Epoch 100/1000\n",
      "4/4 - 1s - loss: 0.1587 - val_loss: 0.0499 - 1s/epoch - 292ms/step\n",
      "Epoch 101/1000\n",
      "4/4 - 1s - loss: 0.1587 - val_loss: 0.0490 - 969ms/epoch - 242ms/step\n",
      "Epoch 102/1000\n",
      "4/4 - 1s - loss: 0.1587 - val_loss: 0.0494 - 952ms/epoch - 238ms/step\n",
      "Epoch 103/1000\n",
      "4/4 - 1s - loss: 0.1583 - val_loss: 0.0498 - 961ms/epoch - 240ms/step\n",
      "Epoch 104/1000\n",
      "4/4 - 1s - loss: 0.1584 - val_loss: 0.0492 - 975ms/epoch - 244ms/step\n",
      "Epoch 105/1000\n",
      "4/4 - 1s - loss: 0.1584 - val_loss: 0.0494 - 981ms/epoch - 245ms/step\n",
      "Epoch 106/1000\n",
      "4/4 - 1s - loss: 0.1585 - val_loss: 0.0492 - 965ms/epoch - 241ms/step\n",
      "Epoch 107/1000\n",
      "4/4 - 1s - loss: 0.1588 - val_loss: 0.0488 - 999ms/epoch - 250ms/step\n",
      "Epoch 108/1000\n",
      "4/4 - 1s - loss: 0.1584 - val_loss: 0.0486 - 1s/epoch - 254ms/step\n",
      "Epoch 109/1000\n",
      "4/4 - 1s - loss: 0.1582 - val_loss: 0.0489 - 960ms/epoch - 240ms/step\n",
      "Epoch 110/1000\n",
      "4/4 - 1s - loss: 0.1583 - val_loss: 0.0492 - 972ms/epoch - 243ms/step\n",
      "Epoch 111/1000\n",
      "4/4 - 1s - loss: 0.1581 - val_loss: 0.0494 - 1s/epoch - 250ms/step\n",
      "Epoch 112/1000\n",
      "4/4 - 1s - loss: 0.1580 - val_loss: 0.0496 - 998ms/epoch - 250ms/step\n",
      "Epoch 113/1000\n",
      "4/4 - 1s - loss: 0.1583 - val_loss: 0.0487 - 1s/epoch - 253ms/step\n",
      "Epoch 114/1000\n",
      "4/4 - 1s - loss: 0.1583 - val_loss: 0.0487 - 1s/epoch - 256ms/step\n",
      "Epoch 115/1000\n",
      "4/4 - 1s - loss: 0.1585 - val_loss: 0.0480 - 1s/epoch - 276ms/step\n",
      "Epoch 116/1000\n",
      "4/4 - 1s - loss: 0.1585 - val_loss: 0.0486 - 1s/epoch - 257ms/step\n",
      "Epoch 117/1000\n",
      "4/4 - 1s - loss: 0.1581 - val_loss: 0.0488 - 1s/epoch - 262ms/step\n",
      "Epoch 118/1000\n",
      "4/4 - 1s - loss: 0.1580 - val_loss: 0.0482 - 1s/epoch - 262ms/step\n",
      "Epoch 119/1000\n",
      "4/4 - 1s - loss: 0.1579 - val_loss: 0.0491 - 1s/epoch - 259ms/step\n",
      "Epoch 120/1000\n",
      "4/4 - 1s - loss: 0.1583 - val_loss: 0.0500 - 1s/epoch - 262ms/step\n",
      "Epoch 121/1000\n",
      "4/4 - 1s - loss: 0.1579 - val_loss: 0.0497 - 1s/epoch - 264ms/step\n",
      "Epoch 122/1000\n",
      "4/4 - 1s - loss: 0.1578 - val_loss: 0.0490 - 1s/epoch - 257ms/step\n",
      "Epoch 123/1000\n",
      "4/4 - 1s - loss: 0.1579 - val_loss: 0.0494 - 1s/epoch - 284ms/step\n",
      "Epoch 124/1000\n",
      "4/4 - 1s - loss: 0.1577 - val_loss: 0.0501 - 1s/epoch - 310ms/step\n",
      "Epoch 125/1000\n",
      "4/4 - 1s - loss: 0.1578 - val_loss: 0.0486 - 1s/epoch - 336ms/step\n",
      "Epoch 126/1000\n",
      "4/4 - 1s - loss: 0.1578 - val_loss: 0.0485 - 1s/epoch - 373ms/step\n",
      "Epoch 127/1000\n",
      "4/4 - 1s - loss: 0.1575 - val_loss: 0.0494 - 1s/epoch - 362ms/step\n",
      "Epoch 128/1000\n",
      "4/4 - 1s - loss: 0.1576 - val_loss: 0.0484 - 1s/epoch - 326ms/step\n",
      "Epoch 129/1000\n",
      "4/4 - 1s - loss: 0.1577 - val_loss: 0.0483 - 1s/epoch - 286ms/step\n",
      "Epoch 130/1000\n",
      "4/4 - 1s - loss: 0.1577 - val_loss: 0.0482 - 1s/epoch - 299ms/step\n",
      "Epoch 131/1000\n",
      "4/4 - 1s - loss: 0.1576 - val_loss: 0.0474 - 1s/epoch - 320ms/step\n",
      "Epoch 132/1000\n",
      "4/4 - 1s - loss: 0.1581 - val_loss: 0.0480 - 1s/epoch - 282ms/step\n",
      "Epoch 133/1000\n",
      "4/4 - 1s - loss: 0.1574 - val_loss: 0.0482 - 1s/epoch - 282ms/step\n",
      "Epoch 134/1000\n",
      "4/4 - 1s - loss: 0.1576 - val_loss: 0.0485 - 1s/epoch - 286ms/step\n",
      "Epoch 135/1000\n",
      "4/4 - 1s - loss: 0.1575 - val_loss: 0.0490 - 1s/epoch - 274ms/step\n",
      "Epoch 136/1000\n",
      "4/4 - 1s - loss: 0.1575 - val_loss: 0.0496 - 1s/epoch - 260ms/step\n",
      "Epoch 137/1000\n",
      "4/4 - 1s - loss: 0.1574 - val_loss: 0.0504 - 1s/epoch - 254ms/step\n",
      "Epoch 138/1000\n",
      "4/4 - 1s - loss: 0.1579 - val_loss: 0.0502 - 1s/epoch - 254ms/step\n",
      "Epoch 139/1000\n",
      "4/4 - 1s - loss: 0.1578 - val_loss: 0.0494 - 1s/epoch - 257ms/step\n",
      "Epoch 140/1000\n",
      "4/4 - 1s - loss: 0.1574 - val_loss: 0.0491 - 1s/epoch - 264ms/step\n",
      "Epoch 141/1000\n",
      "4/4 - 1s - loss: 0.1576 - val_loss: 0.0494 - 1s/epoch - 264ms/step\n",
      "Epoch 142/1000\n",
      "4/4 - 1s - loss: 0.1574 - val_loss: 0.0490 - 1s/epoch - 273ms/step\n",
      "Epoch 143/1000\n",
      "4/4 - 1s - loss: 0.1578 - val_loss: 0.0488 - 1s/epoch - 311ms/step\n",
      "Epoch 144/1000\n",
      "4/4 - 1s - loss: 0.1577 - val_loss: 0.0503 - 1s/epoch - 340ms/step\n",
      "Epoch 145/1000\n",
      "4/4 - 1s - loss: 0.1579 - val_loss: 0.0504 - 1s/epoch - 334ms/step\n",
      "Epoch 146/1000\n",
      "4/4 - 1s - loss: 0.1573 - val_loss: 0.0497 - 1s/epoch - 278ms/step\n",
      "Epoch 147/1000\n",
      "4/4 - 1s - loss: 0.1570 - val_loss: 0.0498 - 1s/epoch - 272ms/step\n",
      "Epoch 148/1000\n",
      "4/4 - 1s - loss: 0.1573 - val_loss: 0.0486 - 1s/epoch - 269ms/step\n",
      "Epoch 149/1000\n",
      "4/4 - 1s - loss: 0.1569 - val_loss: 0.0474 - 1s/epoch - 282ms/step\n",
      "Epoch 150/1000\n",
      "4/4 - 1s - loss: 0.1573 - val_loss: 0.0474 - 1s/epoch - 257ms/step\n",
      "Epoch 151/1000\n",
      "4/4 - 1s - loss: 0.1573 - val_loss: 0.0464 - 1s/epoch - 277ms/step\n",
      "Epoch 152/1000\n",
      "4/4 - 1s - loss: 0.1576 - val_loss: 0.0471 - 1s/epoch - 280ms/step\n",
      "Epoch 153/1000\n",
      "4/4 - 1s - loss: 0.1572 - val_loss: 0.0469 - 1s/epoch - 283ms/step\n",
      "Epoch 154/1000\n",
      "4/4 - 1s - loss: 0.1572 - val_loss: 0.0469 - 1s/epoch - 296ms/step\n",
      "Epoch 155/1000\n",
      "4/4 - 1s - loss: 0.1572 - val_loss: 0.0468 - 1s/epoch - 254ms/step\n",
      "Epoch 156/1000\n",
      "4/4 - 1s - loss: 0.1575 - val_loss: 0.0472 - 1s/epoch - 263ms/step\n",
      "Epoch 157/1000\n",
      "4/4 - 1s - loss: 0.1578 - val_loss: 0.0465 - 1s/epoch - 259ms/step\n",
      "Epoch 158/1000\n",
      "4/4 - 1s - loss: 0.1580 - val_loss: 0.0472 - 1s/epoch - 256ms/step\n",
      "Epoch 159/1000\n",
      "4/4 - 1s - loss: 0.1573 - val_loss: 0.0484 - 1s/epoch - 260ms/step\n",
      "Epoch 160/1000\n",
      "4/4 - 1s - loss: 0.1573 - val_loss: 0.0483 - 1s/epoch - 258ms/step\n",
      "Epoch 161/1000\n",
      "4/4 - 1s - loss: 0.1570 - val_loss: 0.0499 - 1s/epoch - 257ms/step\n",
      "Epoch 162/1000\n",
      "4/4 - 1s - loss: 0.1572 - val_loss: 0.0501 - 1s/epoch - 316ms/step\n",
      "Epoch 163/1000\n",
      "4/4 - 1s - loss: 0.1571 - val_loss: 0.0497 - 1s/epoch - 340ms/step\n",
      "Epoch 164/1000\n",
      "4/4 - 1s - loss: 0.1570 - val_loss: 0.0495 - 1s/epoch - 262ms/step\n",
      "Epoch 165/1000\n",
      "4/4 - 1s - loss: 0.1572 - val_loss: 0.0490 - 1s/epoch - 291ms/step\n",
      "Epoch 166/1000\n",
      "4/4 - 1s - loss: 0.1573 - val_loss: 0.0486 - 1s/epoch - 267ms/step\n",
      "Epoch 167/1000\n",
      "4/4 - 1s - loss: 0.1573 - val_loss: 0.0484 - 1s/epoch - 272ms/step\n",
      "Epoch 168/1000\n",
      "4/4 - 1s - loss: 0.1569 - val_loss: 0.0467 - 1s/epoch - 255ms/step\n",
      "Epoch 169/1000\n",
      "4/4 - 1s - loss: 0.1567 - val_loss: 0.0481 - 1s/epoch - 282ms/step\n",
      "Epoch 170/1000\n",
      "4/4 - 1s - loss: 0.1565 - val_loss: 0.0476 - 1s/epoch - 342ms/step\n",
      "Epoch 171/1000\n",
      "4/4 - 1s - loss: 0.1564 - val_loss: 0.0480 - 1s/epoch - 266ms/step\n",
      "Epoch 172/1000\n",
      "4/4 - 1s - loss: 0.1564 - val_loss: 0.0483 - 1s/epoch - 263ms/step\n",
      "Epoch 173/1000\n",
      "4/4 - 1s - loss: 0.1564 - val_loss: 0.0486 - 1s/epoch - 268ms/step\n",
      "Epoch 174/1000\n",
      "4/4 - 1s - loss: 0.1566 - val_loss: 0.0479 - 1s/epoch - 274ms/step\n",
      "Epoch 175/1000\n",
      "4/4 - 1s - loss: 0.1563 - val_loss: 0.0481 - 1s/epoch - 277ms/step\n",
      "Epoch 176/1000\n",
      "4/4 - 1s - loss: 0.1564 - val_loss: 0.0483 - 1s/epoch - 271ms/step\n",
      "Epoch 177/1000\n",
      "4/4 - 1s - loss: 0.1564 - val_loss: 0.0482 - 1s/epoch - 288ms/step\n",
      "Epoch 178/1000\n",
      "4/4 - 1s - loss: 0.1564 - val_loss: 0.0483 - 1s/epoch - 284ms/step\n",
      "Epoch 179/1000\n",
      "4/4 - 1s - loss: 0.1563 - val_loss: 0.0479 - 1s/epoch - 278ms/step\n",
      "Epoch 180/1000\n",
      "4/4 - 1s - loss: 0.1565 - val_loss: 0.0468 - 1s/epoch - 269ms/step\n",
      "Epoch 181/1000\n",
      "4/4 - 1s - loss: 0.1568 - val_loss: 0.0465 - 1s/epoch - 271ms/step\n",
      "Epoch 182/1000\n",
      "4/4 - 1s - loss: 0.1568 - val_loss: 0.0467 - 1s/epoch - 269ms/step\n",
      "Epoch 183/1000\n",
      "4/4 - 1s - loss: 0.1566 - val_loss: 0.0466 - 1s/epoch - 277ms/step\n",
      "Epoch 184/1000\n",
      "4/4 - 1s - loss: 0.1566 - val_loss: 0.0461 - 1s/epoch - 290ms/step\n",
      "Epoch 185/1000\n",
      "4/4 - 1s - loss: 0.1563 - val_loss: 0.0470 - 1s/epoch - 273ms/step\n",
      "Epoch 186/1000\n",
      "4/4 - 1s - loss: 0.1564 - val_loss: 0.0474 - 1s/epoch - 320ms/step\n",
      "Epoch 187/1000\n",
      "4/4 - 1s - loss: 0.1562 - val_loss: 0.0482 - 1s/epoch - 295ms/step\n",
      "Epoch 188/1000\n",
      "4/4 - 1s - loss: 0.1561 - val_loss: 0.0484 - 1s/epoch - 306ms/step\n",
      "Epoch 189/1000\n",
      "4/4 - 1s - loss: 0.1563 - val_loss: 0.0490 - 1s/epoch - 266ms/step\n",
      "Epoch 190/1000\n",
      "4/4 - 1s - loss: 0.1568 - val_loss: 0.0494 - 1s/epoch - 265ms/step\n",
      "Epoch 191/1000\n",
      "4/4 - 1s - loss: 0.1564 - val_loss: 0.0483 - 1s/epoch - 266ms/step\n",
      "Epoch 192/1000\n",
      "4/4 - 1s - loss: 0.1559 - val_loss: 0.0481 - 1s/epoch - 275ms/step\n",
      "Epoch 193/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0478 - 1s/epoch - 282ms/step\n",
      "Epoch 194/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0476 - 1s/epoch - 281ms/step\n",
      "Epoch 195/1000\n",
      "4/4 - 1s - loss: 0.1558 - val_loss: 0.0469 - 1s/epoch - 274ms/step\n",
      "Epoch 196/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0463 - 1s/epoch - 305ms/step\n",
      "Epoch 197/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0472 - 1s/epoch - 275ms/step\n",
      "Epoch 198/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0475 - 1s/epoch - 294ms/step\n",
      "Epoch 199/1000\n",
      "4/4 - 1s - loss: 0.1562 - val_loss: 0.0475 - 1s/epoch - 314ms/step\n",
      "Epoch 200/1000\n",
      "4/4 - 1s - loss: 0.1561 - val_loss: 0.0475 - 1s/epoch - 325ms/step\n",
      "Epoch 201/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0473 - 1s/epoch - 360ms/step\n",
      "Epoch 202/1000\n",
      "4/4 - 1s - loss: 0.1557 - val_loss: 0.0469 - 1s/epoch - 342ms/step\n",
      "Epoch 203/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0473 - 1s/epoch - 296ms/step\n",
      "Epoch 204/1000\n",
      "4/4 - 1s - loss: 0.1558 - val_loss: 0.0479 - 1s/epoch - 299ms/step\n",
      "Epoch 205/1000\n",
      "4/4 - 1s - loss: 0.1562 - val_loss: 0.0481 - 1s/epoch - 324ms/step\n",
      "Epoch 206/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0472 - 1s/epoch - 331ms/step\n",
      "Epoch 207/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0486 - 1s/epoch - 327ms/step\n",
      "Epoch 208/1000\n",
      "4/4 - 1s - loss: 0.1561 - val_loss: 0.0483 - 1s/epoch - 331ms/step\n",
      "Epoch 209/1000\n",
      "4/4 - 1s - loss: 0.1556 - val_loss: 0.0478 - 1s/epoch - 309ms/step\n",
      "Epoch 210/1000\n",
      "4/4 - 1s - loss: 0.1557 - val_loss: 0.0475 - 1s/epoch - 321ms/step\n",
      "Epoch 211/1000\n",
      "4/4 - 1s - loss: 0.1556 - val_loss: 0.0476 - 1s/epoch - 328ms/step\n",
      "Epoch 212/1000\n",
      "4/4 - 1s - loss: 0.1556 - val_loss: 0.0478 - 1s/epoch - 310ms/step\n",
      "Epoch 213/1000\n",
      "4/4 - 1s - loss: 0.1556 - val_loss: 0.0478 - 1s/epoch - 324ms/step\n",
      "Epoch 214/1000\n",
      "4/4 - 1s - loss: 0.1555 - val_loss: 0.0476 - 1s/epoch - 328ms/step\n",
      "Epoch 215/1000\n",
      "4/4 - 1s - loss: 0.1556 - val_loss: 0.0468 - 1s/epoch - 298ms/step\n",
      "Epoch 216/1000\n",
      "4/4 - 1s - loss: 0.1554 - val_loss: 0.0468 - 1s/epoch - 297ms/step\n",
      "Epoch 217/1000\n",
      "4/4 - 1s - loss: 0.1555 - val_loss: 0.0463 - 1s/epoch - 282ms/step\n",
      "Epoch 218/1000\n",
      "4/4 - 1s - loss: 0.1554 - val_loss: 0.0467 - 1s/epoch - 301ms/step\n",
      "Epoch 219/1000\n",
      "4/4 - 1s - loss: 0.1554 - val_loss: 0.0461 - 1s/epoch - 301ms/step\n",
      "Epoch 220/1000\n",
      "4/4 - 1s - loss: 0.1554 - val_loss: 0.0461 - 1s/epoch - 313ms/step\n",
      "Epoch 221/1000\n",
      "4/4 - 1s - loss: 0.1558 - val_loss: 0.0460 - 1s/epoch - 299ms/step\n",
      "Epoch 222/1000\n",
      "4/4 - 1s - loss: 0.1553 - val_loss: 0.0463 - 1s/epoch - 320ms/step\n",
      "Epoch 223/1000\n",
      "4/4 - 1s - loss: 0.1557 - val_loss: 0.0463 - 1s/epoch - 305ms/step\n",
      "Epoch 224/1000\n",
      "4/4 - 1s - loss: 0.1553 - val_loss: 0.0460 - 1s/epoch - 341ms/step\n",
      "Epoch 225/1000\n",
      "4/4 - 1s - loss: 0.1554 - val_loss: 0.0462 - 1s/epoch - 312ms/step\n",
      "Epoch 226/1000\n",
      "4/4 - 1s - loss: 0.1555 - val_loss: 0.0464 - 1s/epoch - 317ms/step\n",
      "Epoch 227/1000\n",
      "4/4 - 1s - loss: 0.1555 - val_loss: 0.0462 - 1s/epoch - 308ms/step\n",
      "Epoch 228/1000\n",
      "4/4 - 1s - loss: 0.1556 - val_loss: 0.0464 - 1s/epoch - 320ms/step\n",
      "Epoch 229/1000\n",
      "4/4 - 1s - loss: 0.1553 - val_loss: 0.0459 - 1s/epoch - 318ms/step\n",
      "Epoch 230/1000\n",
      "4/4 - 1s - loss: 0.1556 - val_loss: 0.0461 - 1s/epoch - 308ms/step\n",
      "Epoch 231/1000\n",
      "4/4 - 1s - loss: 0.1552 - val_loss: 0.0467 - 1s/epoch - 311ms/step\n",
      "Epoch 232/1000\n",
      "4/4 - 1s - loss: 0.1553 - val_loss: 0.0468 - 1s/epoch - 319ms/step\n",
      "Epoch 233/1000\n",
      "4/4 - 1s - loss: 0.1552 - val_loss: 0.0469 - 1s/epoch - 314ms/step\n",
      "Epoch 234/1000\n",
      "4/4 - 1s - loss: 0.1552 - val_loss: 0.0467 - 1s/epoch - 300ms/step\n",
      "Epoch 235/1000\n",
      "4/4 - 1s - loss: 0.1551 - val_loss: 0.0471 - 1s/epoch - 274ms/step\n",
      "Epoch 236/1000\n",
      "4/4 - 1s - loss: 0.1555 - val_loss: 0.0467 - 1s/epoch - 279ms/step\n",
      "Epoch 237/1000\n",
      "4/4 - 1s - loss: 0.1557 - val_loss: 0.0463 - 1s/epoch - 281ms/step\n",
      "Epoch 238/1000\n",
      "4/4 - 1s - loss: 0.1554 - val_loss: 0.0464 - 1s/epoch - 279ms/step\n",
      "Epoch 239/1000\n",
      "4/4 - 1s - loss: 0.1554 - val_loss: 0.0466 - 1s/epoch - 284ms/step\n",
      "Epoch 240/1000\n",
      "4/4 - 1s - loss: 0.1557 - val_loss: 0.0463 - 1s/epoch - 282ms/step\n",
      "Epoch 241/1000\n",
      "4/4 - 1s - loss: 0.1554 - val_loss: 0.0466 - 1s/epoch - 290ms/step\n",
      "Epoch 242/1000\n",
      "4/4 - 1s - loss: 0.1551 - val_loss: 0.0466 - 1s/epoch - 273ms/step\n",
      "Epoch 243/1000\n",
      "4/4 - 1s - loss: 0.1552 - val_loss: 0.0469 - 1s/epoch - 294ms/step\n",
      "Epoch 244/1000\n",
      "4/4 - 1s - loss: 0.1552 - val_loss: 0.0467 - 1s/epoch - 267ms/step\n",
      "Epoch 245/1000\n",
      "4/4 - 1s - loss: 0.1550 - val_loss: 0.0467 - 1s/epoch - 293ms/step\n",
      "Epoch 246/1000\n",
      "4/4 - 1s - loss: 0.1553 - val_loss: 0.0458 - 1s/epoch - 315ms/step\n",
      "Epoch 247/1000\n",
      "4/4 - 1s - loss: 0.1553 - val_loss: 0.0455 - 1s/epoch - 346ms/step\n",
      "Epoch 248/1000\n",
      "4/4 - 1s - loss: 0.1563 - val_loss: 0.0446 - 1s/epoch - 350ms/step\n",
      "Epoch 249/1000\n",
      "4/4 - 1s - loss: 0.1570 - val_loss: 0.0452 - 1s/epoch - 287ms/step\n",
      "Epoch 250/1000\n",
      "4/4 - 1s - loss: 0.1570 - val_loss: 0.0468 - 1s/epoch - 259ms/step\n",
      "Epoch 251/1000\n",
      "4/4 - 1s - loss: 0.1568 - val_loss: 0.0473 - 1s/epoch - 262ms/step\n",
      "Epoch 252/1000\n",
      "4/4 - 1s - loss: 0.1561 - val_loss: 0.0493 - 1s/epoch - 320ms/step\n",
      "Epoch 253/1000\n",
      "4/4 - 1s - loss: 0.1560 - val_loss: 0.0488 - 1s/epoch - 275ms/step\n",
      "Epoch 254/1000\n",
      "4/4 - 1s - loss: 0.1557 - val_loss: 0.0471 - 1s/epoch - 260ms/step\n",
      "Epoch 255/1000\n",
      "4/4 - 1s - loss: 0.1552 - val_loss: 0.0466 - 1s/epoch - 265ms/step\n",
      "Epoch 256/1000\n",
      "4/4 - 1s - loss: 0.1550 - val_loss: 0.0463 - 1s/epoch - 261ms/step\n",
      "Epoch 257/1000\n",
      "4/4 - 1s - loss: 0.1550 - val_loss: 0.0463 - 1s/epoch - 267ms/step\n",
      "Epoch 258/1000\n",
      "4/4 - 1s - loss: 0.1548 - val_loss: 0.0456 - 1s/epoch - 264ms/step\n",
      "Epoch 259/1000\n",
      "4/4 - 1s - loss: 0.1549 - val_loss: 0.0461 - 1s/epoch - 289ms/step\n",
      "Epoch 260/1000\n",
      "4/4 - 1s - loss: 0.1550 - val_loss: 0.0463 - 1s/epoch - 351ms/step\n",
      "Epoch 261/1000\n",
      "4/4 - 1s - loss: 0.1549 - val_loss: 0.0468 - 1s/epoch - 282ms/step\n",
      "Epoch 262/1000\n",
      "4/4 - 1s - loss: 0.1550 - val_loss: 0.0466 - 1s/epoch - 263ms/step\n",
      "Epoch 263/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0471 - 1s/epoch - 273ms/step\n",
      "Epoch 264/1000\n",
      "4/4 - 1s - loss: 0.1548 - val_loss: 0.0462 - 1s/epoch - 274ms/step\n",
      "Epoch 265/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0456 - 1s/epoch - 279ms/step\n",
      "Epoch 266/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0459 - 1s/epoch - 272ms/step\n",
      "Epoch 267/1000\n",
      "4/4 - 1s - loss: 0.1548 - val_loss: 0.0458 - 1s/epoch - 266ms/step\n",
      "Epoch 268/1000\n",
      "4/4 - 1s - loss: 0.1548 - val_loss: 0.0457 - 1s/epoch - 269ms/step\n",
      "Epoch 269/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0459 - 1s/epoch - 261ms/step\n",
      "Epoch 270/1000\n",
      "4/4 - 1s - loss: 0.1548 - val_loss: 0.0453 - 1s/epoch - 265ms/step\n",
      "Epoch 271/1000\n",
      "4/4 - 1s - loss: 0.1550 - val_loss: 0.0455 - 1s/epoch - 268ms/step\n",
      "Epoch 272/1000\n",
      "4/4 - 1s - loss: 0.1550 - val_loss: 0.0451 - 1s/epoch - 267ms/step\n",
      "Epoch 273/1000\n",
      "4/4 - 1s - loss: 0.1548 - val_loss: 0.0452 - 1s/epoch - 274ms/step\n",
      "Epoch 274/1000\n",
      "4/4 - 1s - loss: 0.1548 - val_loss: 0.0457 - 1s/epoch - 266ms/step\n",
      "Epoch 275/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0457 - 1s/epoch - 260ms/step\n",
      "Epoch 276/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0458 - 1s/epoch - 266ms/step\n",
      "Epoch 277/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0458 - 1s/epoch - 266ms/step\n",
      "Epoch 278/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0456 - 1s/epoch - 264ms/step\n",
      "Epoch 279/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0457 - 1s/epoch - 262ms/step\n",
      "Epoch 280/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0461 - 1s/epoch - 266ms/step\n",
      "Epoch 281/1000\n",
      "4/4 - 1s - loss: 0.1546 - val_loss: 0.0460 - 1s/epoch - 260ms/step\n",
      "Epoch 282/1000\n",
      "4/4 - 1s - loss: 0.1546 - val_loss: 0.0459 - 1s/epoch - 265ms/step\n",
      "Epoch 283/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0465 - 1s/epoch - 266ms/step\n",
      "Epoch 284/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0460 - 1s/epoch - 267ms/step\n",
      "Epoch 285/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0461 - 1s/epoch - 261ms/step\n",
      "Epoch 286/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0460 - 1s/epoch - 259ms/step\n",
      "Epoch 287/1000\n",
      "4/4 - 1s - loss: 0.1544 - val_loss: 0.0461 - 1s/epoch - 259ms/step\n",
      "Epoch 288/1000\n",
      "4/4 - 1s - loss: 0.1544 - val_loss: 0.0460 - 1s/epoch - 256ms/step\n",
      "Epoch 289/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0446 - 1s/epoch - 274ms/step\n",
      "Epoch 290/1000\n",
      "4/4 - 1s - loss: 0.1550 - val_loss: 0.0449 - 1s/epoch - 257ms/step\n",
      "Epoch 291/1000\n",
      "4/4 - 1s - loss: 0.1546 - val_loss: 0.0449 - 1s/epoch - 263ms/step\n",
      "Epoch 292/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0449 - 1s/epoch - 283ms/step\n",
      "Epoch 293/1000\n",
      "4/4 - 1s - loss: 0.1548 - val_loss: 0.0462 - 1s/epoch - 256ms/step\n",
      "Epoch 294/1000\n",
      "4/4 - 1s - loss: 0.1546 - val_loss: 0.0457 - 1s/epoch - 261ms/step\n",
      "Epoch 295/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0466 - 1s/epoch - 262ms/step\n",
      "Epoch 296/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0466 - 1s/epoch - 260ms/step\n",
      "Epoch 297/1000\n",
      "4/4 - 1s - loss: 0.1544 - val_loss: 0.0465 - 1s/epoch - 263ms/step\n",
      "Epoch 298/1000\n",
      "4/4 - 1s - loss: 0.1544 - val_loss: 0.0455 - 1s/epoch - 262ms/step\n",
      "Epoch 299/1000\n",
      "4/4 - 1s - loss: 0.1544 - val_loss: 0.0464 - 1s/epoch - 268ms/step\n",
      "Epoch 300/1000\n",
      "4/4 - 1s - loss: 0.1543 - val_loss: 0.0458 - 1s/epoch - 280ms/step\n",
      "Epoch 301/1000\n",
      "4/4 - 1s - loss: 0.1544 - val_loss: 0.0459 - 1s/epoch - 266ms/step\n",
      "Epoch 302/1000\n",
      "4/4 - 1s - loss: 0.1544 - val_loss: 0.0454 - 1s/epoch - 285ms/step\n",
      "Epoch 303/1000\n",
      "4/4 - 1s - loss: 0.1542 - val_loss: 0.0448 - 1s/epoch - 268ms/step\n",
      "Epoch 304/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0444 - 1s/epoch - 302ms/step\n",
      "Epoch 305/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0446 - 1s/epoch - 341ms/step\n",
      "Epoch 306/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0445 - 1s/epoch - 299ms/step\n",
      "Epoch 307/1000\n",
      "4/4 - 1s - loss: 0.1551 - val_loss: 0.0440 - 1s/epoch - 350ms/step\n",
      "Epoch 308/1000\n",
      "4/4 - 1s - loss: 0.1556 - val_loss: 0.0442 - 1s/epoch - 332ms/step\n",
      "Epoch 309/1000\n",
      "4/4 - 1s - loss: 0.1554 - val_loss: 0.0448 - 1s/epoch - 337ms/step\n",
      "Epoch 310/1000\n",
      "4/4 - 1s - loss: 0.1546 - val_loss: 0.0453 - 1s/epoch - 318ms/step\n",
      "Epoch 311/1000\n",
      "4/4 - 2s - loss: 0.1546 - val_loss: 0.0450 - 2s/epoch - 433ms/step\n",
      "Epoch 312/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0446 - 1s/epoch - 327ms/step\n",
      "Epoch 313/1000\n",
      "4/4 - 1s - loss: 0.1549 - val_loss: 0.0446 - 1s/epoch - 373ms/step\n",
      "Epoch 314/1000\n",
      "4/4 - 1s - loss: 0.1551 - val_loss: 0.0452 - 1s/epoch - 363ms/step\n",
      "Epoch 315/1000\n",
      "4/4 - 1s - loss: 0.1544 - val_loss: 0.0457 - 1s/epoch - 329ms/step\n",
      "Epoch 316/1000\n",
      "4/4 - 1s - loss: 0.1543 - val_loss: 0.0461 - 1s/epoch - 283ms/step\n",
      "Epoch 317/1000\n",
      "4/4 - 1s - loss: 0.1543 - val_loss: 0.0456 - 1s/epoch - 279ms/step\n",
      "Epoch 318/1000\n",
      "4/4 - 1s - loss: 0.1543 - val_loss: 0.0452 - 1s/epoch - 290ms/step\n",
      "Epoch 319/1000\n",
      "4/4 - 1s - loss: 0.1542 - val_loss: 0.0453 - 1s/epoch - 295ms/step\n",
      "Epoch 320/1000\n",
      "4/4 - 1s - loss: 0.1541 - val_loss: 0.0457 - 1s/epoch - 284ms/step\n",
      "Epoch 321/1000\n",
      "4/4 - 1s - loss: 0.1541 - val_loss: 0.0455 - 1s/epoch - 287ms/step\n",
      "Epoch 322/1000\n",
      "4/4 - 2s - loss: 0.1541 - val_loss: 0.0456 - 2s/epoch - 376ms/step\n",
      "Epoch 323/1000\n",
      "4/4 - 1s - loss: 0.1542 - val_loss: 0.0456 - 1s/epoch - 341ms/step\n",
      "Epoch 324/1000\n",
      "4/4 - 2s - loss: 0.1543 - val_loss: 0.0467 - 2s/epoch - 384ms/step\n",
      "Epoch 325/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0465 - 1s/epoch - 310ms/step\n",
      "Epoch 326/1000\n",
      "4/4 - 2s - loss: 0.1543 - val_loss: 0.0465 - 2s/epoch - 376ms/step\n",
      "Epoch 327/1000\n",
      "4/4 - 2s - loss: 0.1541 - val_loss: 0.0456 - 2s/epoch - 422ms/step\n",
      "Epoch 328/1000\n",
      "4/4 - 2s - loss: 0.1540 - val_loss: 0.0463 - 2s/epoch - 384ms/step\n",
      "Epoch 329/1000\n",
      "4/4 - 1s - loss: 0.1540 - val_loss: 0.0466 - 1s/epoch - 291ms/step\n",
      "Epoch 330/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0474 - 1s/epoch - 353ms/step\n",
      "Epoch 331/1000\n",
      "4/4 - 1s - loss: 0.1547 - val_loss: 0.0470 - 1s/epoch - 300ms/step\n",
      "Epoch 332/1000\n",
      "4/4 - 1s - loss: 0.1545 - val_loss: 0.0455 - 1s/epoch - 332ms/step\n",
      "Epoch 333/1000\n",
      "4/4 - 1s - loss: 0.1542 - val_loss: 0.0458 - 1s/epoch - 318ms/step\n",
      "Epoch 334/1000\n",
      "4/4 - 1s - loss: 0.1540 - val_loss: 0.0449 - 1s/epoch - 327ms/step\n",
      "Epoch 335/1000\n",
      "4/4 - 1s - loss: 0.1540 - val_loss: 0.0454 - 1s/epoch - 331ms/step\n",
      "Epoch 336/1000\n",
      "4/4 - 1s - loss: 0.1540 - val_loss: 0.0448 - 1s/epoch - 348ms/step\n",
      "Epoch 337/1000\n",
      "4/4 - 2s - loss: 0.1540 - val_loss: 0.0454 - 2s/epoch - 475ms/step\n",
      "Epoch 338/1000\n",
      "4/4 - 1s - loss: 0.1540 - val_loss: 0.0451 - 1s/epoch - 331ms/step\n",
      "Epoch 339/1000\n",
      "4/4 - 1s - loss: 0.1538 - val_loss: 0.0454 - 1s/epoch - 303ms/step\n",
      "Epoch 340/1000\n",
      "4/4 - 1s - loss: 0.1539 - val_loss: 0.0461 - 1s/epoch - 337ms/step\n",
      "Epoch 341/1000\n",
      "4/4 - 1s - loss: 0.1539 - val_loss: 0.0463 - 1s/epoch - 354ms/step\n",
      "Epoch 342/1000\n",
      "4/4 - 1s - loss: 0.1540 - val_loss: 0.0455 - 1s/epoch - 347ms/step\n",
      "Epoch 343/1000\n",
      "4/4 - 2s - loss: 0.1539 - val_loss: 0.0454 - 2s/epoch - 382ms/step\n",
      "Epoch 344/1000\n",
      "4/4 - 1s - loss: 0.1539 - val_loss: 0.0458 - 1s/epoch - 334ms/step\n",
      "Epoch 345/1000\n",
      "4/4 - 2s - loss: 0.1538 - val_loss: 0.0456 - 2s/epoch - 388ms/step\n",
      "Epoch 346/1000\n",
      "4/4 - 1s - loss: 0.1537 - val_loss: 0.0452 - 1s/epoch - 322ms/step\n",
      "Epoch 347/1000\n",
      "4/4 - 1s - loss: 0.1538 - val_loss: 0.0457 - 1s/epoch - 297ms/step\n",
      "Epoch 348/1000\n",
      "4/4 - 2s - loss: 0.1538 - val_loss: 0.0450 - 2s/epoch - 467ms/step\n",
      "Epoch 349/1000\n",
      "4/4 - 1s - loss: 0.1537 - val_loss: 0.0462 - 1s/epoch - 297ms/step\n",
      "Epoch 350/1000\n",
      "4/4 - 1s - loss: 0.1538 - val_loss: 0.0453 - 1s/epoch - 282ms/step\n",
      "Epoch 351/1000\n",
      "4/4 - 1s - loss: 0.1538 - val_loss: 0.0452 - 1s/epoch - 277ms/step\n",
      "Epoch 352/1000\n",
      "4/4 - 1s - loss: 0.1536 - val_loss: 0.0455 - 1s/epoch - 322ms/step\n",
      "Epoch 353/1000\n",
      "4/4 - 1s - loss: 0.1537 - val_loss: 0.0456 - 1s/epoch - 274ms/step\n",
      "Epoch 354/1000\n",
      "4/4 - 1s - loss: 0.1537 - val_loss: 0.0453 - 1s/epoch - 264ms/step\n",
      "Epoch 355/1000\n",
      "4/4 - 1s - loss: 0.1537 - val_loss: 0.0455 - 1s/epoch - 259ms/step\n",
      "Epoch 356/1000\n",
      "4/4 - 1s - loss: 0.1538 - val_loss: 0.0452 - 1s/epoch - 266ms/step\n",
      "Epoch 357/1000\n",
      "4/4 - 1s - loss: 0.1539 - val_loss: 0.0453 - 1s/epoch - 278ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>random_recall_at_10</td><td>▁</td></tr><tr><td>random_recall_at_100</td><td>▁</td></tr><tr><td>random_recall_at_5</td><td>▁</td></tr><tr><td>random_recall_at_50</td><td>▁</td></tr><tr><td>recall_at_10</td><td>▁</td></tr><tr><td>recall_at_100</td><td>▁</td></tr><tr><td>recall_at_5</td><td>▁</td></tr><tr><td>recall_at_50</td><td>▁</td></tr><tr><td>val_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.15387</td></tr><tr><td>random_recall_at_10</td><td>0.00986</td></tr><tr><td>random_recall_at_100</td><td>0.0594</td></tr><tr><td>random_recall_at_5</td><td>0.00287</td></tr><tr><td>random_recall_at_50</td><td>0.02588</td></tr><tr><td>recall_at_10</td><td>0.05616</td></tr><tr><td>recall_at_100</td><td>0.35695</td></tr><tr><td>recall_at_5</td><td>0.03409</td></tr><tr><td>recall_at_50</td><td>0.21627</td></tr><tr><td>val_loss</td><td>0.04529</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dataset_ml_100k, trial_0, latent_8, layers_2, regularizer_factor_0.01, dropout_rate_0.1, use_batch_norm_False</strong> at: <a href='https://wandb.ai/munchong915/autoencoder_optimization/runs/8ejqhifn' target=\"_blank\">https://wandb.ai/munchong915/autoencoder_optimization/runs/8ejqhifn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231216_195455-8ejqhifn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:02:03,853] Trial 0 finished with value: -0.056163947160317085 and parameters: {'latent_dim': 8, 'num_layers': 2, 'regularizer_factor': 0.01, 'dropout_rate': 0.1, 'use_batch_norm': False}. Best is trial 0 with value: -0.056163947160317085.\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m model_type_to_tune \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mGridSampler(search_space))\n\u001b[0;32m---> 17\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type_to_tune\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_bar_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Allow Optuna to run through all combinations in the grid\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mwandb_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Close wandb run\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# wandb.finish()\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/optuna/study/_optimize.py:174\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[0;32m--> 174\u001b[0m         \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrozen_trial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     elapsed_seconds \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m time_start)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mwandb_callback\u001b[0;34m(study, trial)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwandb_callback\u001b[39m(study, trial):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Log trial metrics and parameters to wandb\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrial_number\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrial_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "# wandb.init(project=\"autoencoder_optimization\")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "search_space = {\n",
    "    'latent_dim': [2, 4, 8, 16, 32, 64, 128, 256],\n",
    "    'num_layers': list(range(1, 6)),\n",
    "    'regularizer_factor': [0, 1e-2],\n",
    "    'dropout_rate': [0.0, 0.1, 0.2, 0.3],\n",
    "    'use_batch_norm': [True, False]\n",
    "}\n",
    "\n",
    "# Create the study with GridSampler\n",
    "model_type_to_tune = 'user'\n",
    "\n",
    "study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space))\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, dataset, p_value, model_type_to_tune, R_bar_train, R_bar_val),\n",
    "    n_trials=None,  # Allow Optuna to run through all combinations in the grid\n",
    ")\n",
    "# Close wandb run\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
