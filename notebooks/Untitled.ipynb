{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137751c6-1f78-4944-b3a9-783a0699ee96",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "546205a2-3ff4-439f-96ab-3bcd4b697153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/munchong/Desktop/recsys/implicit_feedback_recsys/env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from scipy import sparse\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def compute_rmse(dataset, model, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Computes RMSE for the entire dataset using the provided model.\n",
    "    \n",
    "    Args:\n",
    "    - dataset (list): List of subgraphs for evaluation.\n",
    "    - model (IGMC): The trained IGMC model.\n",
    "    \n",
    "    Returns:\n",
    "    - RMSE (float): Root Mean Squared Error for the dataset.\n",
    "    \"\"\"\n",
    "    # Iterate over batches\n",
    "    num_batches = len(dataset) // batch_size\n",
    "    \n",
    "    total_squared_error = 0\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_subgraphs = dataset[start_idx:end_idx]\n",
    "        \n",
    "        # Batch the entire dataset\n",
    "        processed_data = pyg_batching(batch_subgraphs)\n",
    "\n",
    "        # Predict ratings for the entire dataset\n",
    "        H_concat = model([processed_data['x'], processed_data['edge_index'], processed_data['edge_type'], processed_data['batch']])\n",
    "        predicted_ratings_all = model.mlp(H_concat)\n",
    "\n",
    "        # Extract the predicted ratings for the central nodes\n",
    "#         unique_values, _ = tf.unique(processed_data['batch'])\n",
    "#         central_node_indices = tf.stack([tf.where(processed_data['batch'] == val)[0][0] for val in unique_values])\n",
    "#         predicted_ratings_central = tf.gather(predicted_ratings_all, central_node_indices)\n",
    "#         predicted_ratings_central = tf.squeeze(predicted_ratings_central, axis=-1)\n",
    "\n",
    "        # Compute the squared error for the entire dataset\n",
    "        squared_error = tf.math.square(predicted_ratings_all - processed_data['y'])\n",
    "        \n",
    "        total_squared_error += tf.reduce_sum(squared_error).numpy()\n",
    "        \n",
    "    rmse = np.sqrt(total_squared_error / (num_batches * batch_size))\n",
    "\n",
    "    return rmse\n",
    "\n",
    "def parallel_func(args):\n",
    "    return construct_tf_graph(*subgraph_extraction_labeling_tf(*args))\n",
    "\n",
    "def apply_edge_dropout(subgraph, dropout_prob=0.2):\n",
    "\n",
    "    edge_index_np = subgraph['edge_index'].numpy()\n",
    "    edge_type_np = subgraph['edge_type'].numpy()\n",
    "\n",
    "    # Generate a mask for edges to keep based on the dropout probability\n",
    "    mask = np.random.rand(edge_index_np.shape[1]) > dropout_prob\n",
    "\n",
    "    # Apply the mask to the edge_index and edge_type arrays\n",
    "    edge_index_np = edge_index_np[:, mask]\n",
    "    edge_type_np = edge_type_np[mask]\n",
    "\n",
    "    subgraph['edge_index'] = tf.convert_to_tensor(edge_index_np, dtype=tf.int64)\n",
    "    subgraph['edge_type'] = tf.convert_to_tensor(edge_type_np, dtype=tf.int64)\n",
    "\n",
    "    return subgraph\n",
    "\n",
    "\n",
    "class MyDataset:\n",
    "    def __init__(self, Arow, Acol, links, labels, h=1, sample_ratio=1.0, max_nodes_per_hop=None, processed_file_path='./processed_files/data', parallel=False):\n",
    "        self.Arow = Arow\n",
    "        self.Acol = Acol\n",
    "        self.links = links\n",
    "        self.labels = labels\n",
    "        self.h = h\n",
    "        self.sample_ratio = sample_ratio\n",
    "        self.max_nodes_per_hop = max_nodes_per_hop\n",
    "        self.processed_file_path = processed_file_path\n",
    "        self.parallel = parallel\n",
    "\n",
    "    def links2subgraphs(self):\n",
    "        \n",
    "        print('Enclosing subgraph extraction begins...')\n",
    "        \n",
    "        # Define the arguments for each function call in the parallel processing\n",
    "        args = [\n",
    "            ((i, j), self.Arow, self.Acol, g_label, self.h, self.sample_ratio, self.max_nodes_per_hop)\n",
    "                for i, j, g_label in zip(self.links[0], self.links[1], self.labels)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        if not self.parallel:\n",
    "            g_list = []\n",
    "            with tqdm(total=len(self.links[0])) as pbar:\n",
    "                for i, j, g_label in zip(self.links[0], self.links[1], self.labels):\n",
    "                    tmp = subgraph_extraction_labeling_tf(\n",
    "                        (i, j), self.Arow, self.Acol, g_label, self.h, self.sample_ratio, self.max_nodes_per_hop\n",
    "                    )\n",
    "                    data = construct_tf_graph(*tmp)\n",
    "\n",
    "                    g_list.append(data)\n",
    "                    pbar.update(1)\n",
    "\n",
    "            return g_list\n",
    "        # else:\n",
    "        #     start = time.time()\n",
    "        #     with mp.Pool(mp.cpu_count()) as pool:\n",
    "        #         results = pool.map_async(parallel_func, args)\n",
    "\n",
    "        #         remaining = len(args)\n",
    "        #         pbar = tqdm(total=remaining)\n",
    "        #         while not results.ready():\n",
    "        #             remaining_new = results._number_left\n",
    "        #             pbar.update(remaining - remaining_new)\n",
    "        #             remaining = remaining_new\n",
    "        #             time.sleep(1)\n",
    "\n",
    "        #         results = results.get()\n",
    "        #         pool.close()\n",
    "\n",
    "        #     pbar.close()\n",
    "        #     end = time.time()\n",
    "        #     print(f\"Time elapsed for subgraph extraction: {end-start}s\")\n",
    "\n",
    "        #     return results\n",
    "    \n",
    "    def process(self):\n",
    "        # Extract enclosing subgraphs\n",
    "        data_list = self.links2subgraphs()\n",
    "        \n",
    "        # # Serialize and save the data_list using pickle\n",
    "        # os.makedirs(os.path.dirname(self.processed_file_path), exist_ok=True)\n",
    "        # with open(self.processed_file_path, 'wb') as f:\n",
    "        #     pickle.dump(data_list, f)\n",
    "            \n",
    "        # del data_list\n",
    "        return data_list\n",
    "\n",
    "    def load(self):\n",
    "        \n",
    "        with open(self.processed_file_path, 'rb') as f:\n",
    "            loaded_data_list = pickle.load(f)\n",
    "        \n",
    "        return loaded_data_list\n",
    "\n",
    "class SparseRowIndexer:\n",
    "    def __init__(self, csr_matrix):\n",
    "        data = []\n",
    "        indices = []\n",
    "        indptr = []\n",
    "\n",
    "        for row_start, row_end in zip(csr_matrix.indptr[:-1], csr_matrix.indptr[1:]):\n",
    "            data.append(csr_matrix.data[row_start:row_end])\n",
    "            indices.append(csr_matrix.indices[row_start:row_end])\n",
    "            indptr.append(row_end - row_start)  # nnz of the row\n",
    "\n",
    "        self.data = np.array(data, dtype=object)\n",
    "        self.indices = np.array(indices, dtype=object)\n",
    "        self.indptr = np.array(indptr, dtype=object)\n",
    "        self.shape = csr_matrix.shape\n",
    "\n",
    "    def __getitem__(self, row_selector):\n",
    "        indices = np.concatenate(self.indices[row_selector])\n",
    "        data = np.concatenate(self.data[row_selector])\n",
    "        indptr = np.append(0, np.cumsum(self.indptr[row_selector]))\n",
    "        shape = [indptr.shape[0] - 1, self.shape[1]]\n",
    "        return sparse.csr_matrix((data, indices, indptr), shape=shape)\n",
    "\n",
    "class SparseColIndexer:\n",
    "    def __init__(self, csc_matrix):\n",
    "        data = []\n",
    "        indices = []\n",
    "        indptr = []\n",
    "\n",
    "        for col_start, col_end in zip(csc_matrix.indptr[:-1], csc_matrix.indptr[1:]):\n",
    "            data.append(csc_matrix.data[col_start:col_end])\n",
    "            indices.append(csc_matrix.indices[col_start:col_end])\n",
    "            indptr.append(col_end - col_start)\n",
    "\n",
    "        self.data = np.array(data, dtype=object)\n",
    "        self.indices = np.array(indices, dtype=object)\n",
    "        self.indptr = np.array(indptr, dtype=object)\n",
    "        self.shape = csc_matrix.shape\n",
    "\n",
    "    def __getitem__(self, col_selector):\n",
    "        indices = np.concatenate(self.indices[col_selector])\n",
    "        data = np.concatenate(self.data[col_selector])\n",
    "        indptr = np.append(0, np.cumsum(self.indptr[col_selector]))\n",
    "\n",
    "        shape = [self.shape[0], indptr.shape[0] - 1]\n",
    "        return sparse.csc_matrix((data, indices, indptr), shape=shape)\n",
    "\n",
    "\n",
    "def one_hot(indices, depth):\n",
    "    return tf.one_hot(indices, depth)\n",
    "\n",
    "def construct_tf_graph(u, v, r, node_labels, max_node_label, y):\n",
    "    u, v = tf.convert_to_tensor(u, dtype=tf.float32), tf.convert_to_tensor(v, dtype=tf.float32)\n",
    "    r = tf.convert_to_tensor(r, dtype=tf.float32)\n",
    "    edge_index = tf.stack([tf.concat([u, v], axis=0), tf.concat([v, u], axis=0)], axis=0)\n",
    "    edge_type = tf.concat([r, r], axis=0)\n",
    "    x = tf.cast(one_hot(node_labels, max_node_label+1), dtype=tf.float32)\n",
    "    y = tf.convert_to_tensor([y], dtype=tf.float32)\n",
    "    \n",
    "    data = {\n",
    "        'x': x,\n",
    "        'edge_index': edge_index,\n",
    "        'edge_type': edge_type,\n",
    "        'y': y\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "def neighbors(fringe, A):\n",
    "    # find all 1-hop neighbors of nodes in fringe from A\n",
    "    if not fringe:\n",
    "        return set([])\n",
    "    return set(A[list(fringe)].indices)\n",
    "\n",
    "def subgraph_extraction_labeling_tf(ind, Arow, Acol, y, h=1, sample_ratio=1.0, max_nodes_per_hop=None):\n",
    "    # extract the h-hop enclosing subgraph around link 'ind'\n",
    "    u_nodes, v_nodes = [ind[0]], [ind[1]]\n",
    "    u_dist, v_dist = [0], [0]\n",
    "    u_visited, v_visited = set([ind[0]]), set([ind[1]])\n",
    "    u_fringe, v_fringe = set([ind[0]]), set([ind[1]])\n",
    "    \n",
    "    for dist in range(1, h+1):\n",
    "        v_fringe, u_fringe = neighbors(u_fringe, Arow), neighbors(v_fringe, Acol)\n",
    "        u_fringe = u_fringe - u_visited\n",
    "        v_fringe = v_fringe - v_visited\n",
    "        u_visited = u_visited.union(u_fringe)\n",
    "        v_visited = v_visited.union(v_fringe)\n",
    "        if sample_ratio < 1.0:\n",
    "            u_fringe = random.sample(u_fringe, int(sample_ratio*len(u_fringe)))\n",
    "            v_fringe = random.sample(v_fringe, int(sample_ratio*len(v_fringe)))\n",
    "        if max_nodes_per_hop is not None:\n",
    "            if max_nodes_per_hop < len(u_fringe):\n",
    "                u_fringe = random.sample(u_fringe, max_nodes_per_hop)\n",
    "            if max_nodes_per_hop < len(v_fringe):\n",
    "                v_fringe = random.sample(v_fringe, max_nodes_per_hop)\n",
    "        if len(u_fringe) == 0 and len(v_fringe) == 0:\n",
    "            break\n",
    "        u_nodes = u_nodes + list(u_fringe)\n",
    "        v_nodes = v_nodes + list(v_fringe)\n",
    "        u_dist = u_dist + [dist] * len(u_fringe)\n",
    "        v_dist = v_dist + [dist] * len(v_fringe)\n",
    "        \n",
    "    subgraph = Arow[u_nodes][:, v_nodes]\n",
    "    subgraph[0, 0] = 0\n",
    "    \n",
    "    # prepare pyg graph constructor input\n",
    "    u, v, r = sparse.find(subgraph)  # r is 1, 2... (rating labels + 1)\n",
    "    v += len(u_nodes)\n",
    "    r = r - 1  # transform r back to rating label\n",
    "    num_nodes = len(u_nodes) + len(v_nodes)\n",
    "    node_labels = [x*2 for x in u_dist] + [x*2+1 for x in v_dist]\n",
    "    max_node_label = 2*h + 1\n",
    "    \n",
    "    return u, v, r, node_labels, max_node_label, y\n",
    "\n",
    "def pyg_batching(subgraphs):\n",
    "    \"\"\"\n",
    "    Create a PyG-like batched representation from a list of subgraphs.\n",
    "    \n",
    "    Parameters:\n",
    "    - subgraphs: List of subgraphs\n",
    "    \n",
    "    Returns:\n",
    "    - batched_data: Dictionary containing batched node features, edge indices, edge types, and batch vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lists to store batched data\n",
    "    batched_x = []\n",
    "    batched_edge_index = []\n",
    "    batched_edge_type = []\n",
    "    batch_vector = []\n",
    "    batched_y = []\n",
    "    \n",
    "    # Variables to keep track of the number of nodes seen so far\n",
    "    nodes_cumsum = 0\n",
    "    \n",
    "    for subgraph in subgraphs:\n",
    "        # Node features\n",
    "        x = subgraph['x'].numpy()\n",
    "        batched_x.append(x)\n",
    "        \n",
    "        y = subgraph['y'].numpy()\n",
    "        batched_y.append(y)\n",
    "        \n",
    "        # Edge indices (adjusted based on the number of nodes seen so far)\n",
    "        edge_index = subgraph['edge_index'].numpy() + nodes_cumsum\n",
    "        batched_edge_index.append(edge_index)\n",
    "        \n",
    "        # Edge types\n",
    "        edge_type = subgraph['edge_type'].numpy()\n",
    "        batched_edge_type.append(edge_type)\n",
    "        \n",
    "        # Batch vector\n",
    "        batch_vector.extend([len(batched_x) - 1] * x.shape[0])\n",
    "        \n",
    "        # Update nodes_cumsum\n",
    "        nodes_cumsum += x.shape[0]\n",
    "    \n",
    "    # Concatenate everything to form the batched data\n",
    "    batched_data = {\n",
    "        'x': tf.convert_to_tensor(np.concatenate(batched_x, axis=0), dtype=tf.float32),\n",
    "        'edge_index': tf.convert_to_tensor(np.concatenate(batched_edge_index, axis=1), dtype=tf.int32),\n",
    "        'edge_type': tf.convert_to_tensor(np.concatenate(batched_edge_type), dtype=tf.int32),\n",
    "        'y': tf.convert_to_tensor(np.concatenate(batched_y, axis=0), dtype=tf.float32),\n",
    "        'batch': tf.convert_to_tensor(batch_vector, dtype=tf.int32)\n",
    "    }\n",
    "    \n",
    "    return batched_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c71aa31-925b-45ee-a89a-3451c4b6a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Load the data\n",
    "# Replace 'path_to_ratings.dat' with the actual path to your 'ratings.dat' file\n",
    "ratings_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv('data/ml-100k/u.data', sep='\\t', header=None, names=ratings_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dd0cc34-2761-40bf-852f-d52e1b1534af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>unix_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating  unix_timestamp\n",
       "0      196       242       3       881250949\n",
       "1      186       302       3       891717742\n",
       "2       22       377       1       878887116\n",
       "3      244        51       2       880606923\n",
       "4      166       346       1       886397596"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feadba8d-8040-4a24-859a-206ff7899489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Encode User and Movie IDs\n",
    "user_encoder = LabelEncoder()\n",
    "movie_encoder = LabelEncoder()\n",
    "\n",
    "ratings['user_id'] = user_encoder.fit_transform(ratings['user_id'])\n",
    "ratings['movie_id'] = movie_encoder.fit_transform(ratings['movie_id'])\n",
    "\n",
    "# Step 3: Create Pivot Table\n",
    "pivot_df = ratings.pivot_table(values='rating', index='user_id', columns='movie_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc66a4fd-1d0b-4332-86b4-8a499cb5de9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3400251-e289-4073-8685-829008aed2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movie_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1672</th>\n",
       "      <th>1673</th>\n",
       "      <th>1674</th>\n",
       "      <th>1675</th>\n",
       "      <th>1676</th>\n",
       "      <th>1677</th>\n",
       "      <th>1678</th>\n",
       "      <th>1679</th>\n",
       "      <th>1680</th>\n",
       "      <th>1681</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>943 rows × 1682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movie_id  0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "user_id                                                               ...   \n",
       "0          5.0   3.0   4.0   3.0   3.0   5.0   4.0   1.0   5.0   3.0  ...   \n",
       "1          4.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   2.0  ...   \n",
       "2          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4          4.0   3.0   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "938        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   5.0   NaN  ...   \n",
       "939        NaN   NaN   NaN   2.0   NaN   NaN   4.0   5.0   3.0   NaN  ...   \n",
       "940        5.0   NaN   NaN   NaN   NaN   NaN   4.0   NaN   NaN   NaN  ...   \n",
       "941        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "942        NaN   5.0   NaN   NaN   NaN   NaN   NaN   NaN   3.0   NaN  ...   \n",
       "\n",
       "movie_id  1672  1673  1674  1675  1676  1677  1678  1679  1680  1681  \n",
       "user_id                                                               \n",
       "0          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4          NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "938        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "939        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "940        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "941        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "942        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[943 rows x 1682 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac57955a-d357-47e1-9d50-434d800065f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import prepare_data\n",
    "\n",
    "R_bar_train, R_bar_val, R_bar_test, R_train, R_val, R_test = prepare_data('ml_100k', p = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e792fd5-861c-4473-b8b7-d2e13edc0911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([943, 1682])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_bar_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02cb2916-198a-4ed9-b822-ebbfad25824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eb48cc9-698d-4ecc-9235-698cd18e7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train_csr = csr_matrix(R_train)\n",
    "R_train_csc = csc_matrix(R_train)\n",
    "\n",
    "Arow_train = SparseRowIndexer(R_train_csr)\n",
    "Acol_train = SparseColIndexer(R_train_csc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d900cbd4-1af7-44ba-aa44-cc2fb72f3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows, train_cols, train_labels = sparse.find(R_train_csr)\n",
    "train_indices = np.row_stack((train_rows, train_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "558fdfa8-c043-4a0b-8393-157cbcf83ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 942, 942, 942], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8de4700d-14a7-49b0-80f3-e8c2515178b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  942,  942,  942],\n",
       "       [   0,    1,    2, ..., 1066, 1073, 1187]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308ec69-109b-4798-bca3-fdcb4e49368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file_path = f'./processed_files/IGMC_data/p={args.p}'\n",
    "train_dataset = MyDataset(Arow_train, Acol_train, train_indices, train_labels, processed_file_path=f'{processed_file_path}/train_set')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ce432-687f-4610-836a-c505e37a390e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbc7d2-c95f-477a-acb6-9ec686423b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b35f3-956e-46a9-99fd-b0f1766aacba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216b45bc-fd64-4918-8821-f369a9ef0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "R_train = np.array([\n",
    "    [2, 0, 1, 0, 0],  # User 1\n",
    "    [3, 1, 0, 0, 5],  # User 2\n",
    "    [0, 0, 1, 0, 3],  # User 3\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b4677d-cf3f-4cdc-a9c1-ec2d02e76e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "\n",
    "R_train_csr = csr_matrix(R_train)\n",
    "R_train_csc = csc_matrix(R_train)\n",
    "Arow_train = SparseRowIndexer(R_train_csr)\n",
    "Acol_train = SparseColIndexer(R_train_csc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11db9156-c9b4-4cfb-940c-a21f85ca15c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39294ba-8eb7-4aa9-afff-029189a0bcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SparseRowIndexer at 0x1089211c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arow_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f418337-8a71-4678-a4a5-14bc1595ab8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 1, 5, 1, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train_csr.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fafbe35-3da2-4912-98d0-b483d5733839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1, 4, 2, 4], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train_csr.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427a3afc-1a3f-4123-84d7-636e79dafe92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 5, 7], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train_csr.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "083baa2b-85df-478b-9e0b-439aecba2a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rows, train_cols, train_labels = sparse.find(R_train_csr)\n",
    "train_indices = np.row_stack((train_rows, train_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d657d42-b763-4298-8cb8-f8b8ac639ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 2, 2], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae71680c-5508-43d5-a09f-b155c445e475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1, 4, 2, 4], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c2ec73b-4a71-4da3-95e0-bb28a75344d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 1, 5, 1, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1efe3afa-bac1-4931-bede-cfab224bed44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 2, 2],\n",
       "       [0, 2, 0, 1, 4, 2, 4]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31ebbd6c-77b5-494a-ac33-f533743d6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file_path = f'./processed_files/IGMC_data/p={0.0}'\n",
    "\n",
    "train_dataset = MyDataset(Arow_train, Acol_train, train_indices, train_labels, processed_file_path=f'{processed_file_path}/train_set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d9bcdc1-50f4-452f-93c8-8f67c1df49f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enclosing subgraph extraction begins...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                  | 0/7 [00:00<?, ?it/s]2024-02-25 20:53:01.679754: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 83.77it/s]\n"
     ]
    }
   ],
   "source": [
    "train_subgraphs = train_dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "414a2129-81ee-46fb-9220-dd85876127ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_subgraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94c3845f-3e64-43f8-a4bb-ad9e830a1530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       " array([[1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.]], dtype=float32)>,\n",
       " 'edge_index': <tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       " array([[0., 1., 3., 2.],\n",
       "        [3., 2., 0., 1.]], dtype=float32)>,\n",
       " 'edge_type': <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 2., 0., 2.], dtype=float32)>,\n",
       " 'y': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.], dtype=float32)>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subgraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1ddded4-25c6-467f-82c3-5a13dd7d2acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 1, 0, 0],\n",
       "       [3, 1, 0, 0, 5],\n",
       "       [0, 0, 1, 0, 3]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b0cdd-af99-4b02-8982-34a3ed3ee089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4dd20-48ed-43cb-b3a9-b506b42067d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97432f8a-9858-4f84-99e2-2d351ad7a5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43311258-5457-4078-88d0-fcb643d32ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dummy user-item interaction matrix\n",
    "# Assume 5 users and 5 items for simplicity\n",
    "# 1 indicates interaction, 0 indicates no interaction\n",
    "\n",
    "user_item_matrix = np.array([\n",
    "    [2, 0, 1, 0, 1],  # User 1\n",
    "    [3, 1, 0, 0, 0],  # User 2\n",
    "    [0, 1, 1, 1, 0],  # User 3\n",
    "])\n",
    "\n",
    "# Convert to CSR (Compressed Sparse Row) format for efficient processing\n",
    "user_item_csr = sp.csr_matrix(user_item_matrix)\n",
    "\n",
    "user_item_csr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117239a-322a-4f33-b3dd-728aae583264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345fbc6-f2b6-4a12-9100-f74b22ecc7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1ec76f-38d2-4a8f-bec9-625618883045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9290aa7-eba3-4a75-b4e0-b8fc9f44bbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c37095-9e34-4d35-8d95-5d91fed73ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6abfc4-45a6-4cd8-a3f3-c2d42bcb1ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b12a8-de50-485e-9f8e-d676b292e7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "implicit_feedback_env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
